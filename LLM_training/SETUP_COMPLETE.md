# âœ… SETUP COMPLETO - PRONTO PARA TREINO

## Resumo do Que Foi Feito

### 1. âœ… Dataset Preparado
- **Total:** 943 exemplos
- **Treino:** 848 exemplos (89.9%)
- **ValidaÃ§Ã£o:** 95 exemplos (10.1%)
- **Formato:** JSONL com estrutura prompt/completion
- **Limpeza:** NormalizaÃ§Ã£o de Ã©pocas realizada
- **ValidaÃ§Ã£o:** 100% de registos vÃ¡lidos

**Ficheiros:**
- `data/train.jsonl` - Dataset de treino (pronto)
- `data/valid.jsonl` - Dataset de validaÃ§Ã£o (pronto)

### 2. âœ… Notebook Otimizado Criado
**Ficheiro:** `notebooks/mistral_qlora_training_m1_optimized.ipynb`

Este notebook estÃ¡ especificamente otimizado para seu **MacBook Pro M1 16GB** com:
- âœ“ Batch size = 4 (seguro para M1 16GB)
- âœ“ Gradient accumulation = 2 (effective batch = 8)
- âœ“ Learning rate = 2e-4 (Ã³timo para LoRA)
- âœ“ 3 Ã©pocas (bom para dataset de 943 exemplos)
- âœ“ Sistema completo de mÃ©tricas e logging
- âœ“ Checkpointing automÃ¡tico para recuperaÃ§Ã£o

### 3. âœ… DocumentaÃ§Ã£o Criada

#### DocumentaÃ§Ã£o de ConfiguraÃ§Ã£o
- **`M1_16GB_OPTIMIZATION.md`** - Guia detalhado de otimizaÃ§Ãµes
  - ExplicaÃ§Ã£o de cada parÃ¢metro
  - Uso de memÃ³ria esperado
  - TrajetÃ³ria de loss prevista
  - Troubleshooting completo

- **`CONFIG_SUMMARY.txt`** - Resumo visual das configuraÃ§Ãµes
  - FÃ¡cil de consultar durante treino
  - ParÃ¢metros principais highlighted
  - Checklist prÃ©-treino

- **`START_TRAINING_M1.md`** - Quick start em 30 segundos
  - InstruÃ§Ãµes diretas
  - Sem complicaÃ§Ãµes
  - Pronto para executar

#### DocumentaÃ§Ã£o do Dataset
- **`DATASET_PREPARED.md`** - AnÃ¡lise completa do dataset
  - EstatÃ­sticas detalhadas
  - DistribuiÃ§Ã£o de dados
  - InformaÃ§Ãµes de qualidade
  - RecomendaÃ§Ãµes

#### DocumentaÃ§Ã£o do Projeto
- **`CLAUDE.md`** - Guia para futuros Claude Code
  - Arquitetura completa
  - Como estender
  - Troubleshooting
  - Comandos comuns

### 4. âœ… Scripts Auxiliares

#### Script de Split Atualizado
- **`scripts/split_data_proper.py`** - Novo script de split
  - MantÃ©m formato JSONL intacto
  - Split 90/10 reproducÃ­vel
  - Seed 42 para determinismo

#### Scripts JÃ¡ DisponÃ­veis
- `scripts/train_qlora.py` - Treino via script (alternativa)
- `scripts/inference_qlora.py` - Testar modelo
- `scripts/monitor.py` - Monitoramento em tempo real
- `scripts/visualization.py` - Gerar grÃ¡ficos
- `scripts/clean_dataset.py` - Limpeza de dados (jÃ¡ usado)

---

## ğŸ¯ ConfiguraÃ§Ãµes M1 16GB (Resumo)

### Batch Size
```
batch_size = 4
gradient_accumulation_steps = 2
Effective Batch Size = 8

MemÃ³ria usada: ~3-4 GB por batch
Total mÃ¡ximo: ~10-11 GB (de 16 GB disponÃ­veis)
```

### Learning & Training
```
Learning Rate: 2e-4
Warmup Steps: 100
Num Epochs: 3
Max Seq Length: 512
```

### LoRA Configuration
```
Rank: 8
Scale: 16
Target Modules: 7 (q_proj, v_proj, k_proj, o_proj, gate_proj, up_proj, down_proj)
```

### Checkpointing
```
Save Checkpoint: Every 200 steps
Validate: Every 200 steps
Log: Every 10 steps
```

---

## ğŸ“Š Tempo Estimado

| Fase | DuraÃ§Ã£o | Notas |
|------|---------|-------|
| Setup & VerificaÃ§Ã£o | 2-3 min | Carregar imports e dados |
| Load Modelo | 1-2 min | Primeira vez pode ser mais lenta |
| TokenizaÃ§Ã£o | 1 min | Converter texto em tokens |
| **TREINO Ã‰POCA 1** | **~40 min** | Loss: 4.5 â†’ 3.0 |
| **TREINO Ã‰POCA 2** | **~40 min** | Loss: 3.0 â†’ 2.0 |
| **TREINO Ã‰POCA 3** | **~40 min** | Loss: 2.0 â†’ 1.5 |
| Testes & Export | 5 min | ValidaÃ§Ã£o final |
| **TOTAL** | **~2-3 horas** | Tempo total de execuÃ§Ã£o |

---

## ğŸš€ Como ComeÃ§ar

### Passo 1: PrÃ©-requisitos (5 min)

```bash
# Verificar Python
python3 --version  # Deve ser 3.11+

# Verificar GPU
python3 -c "import mlx.core as mx; print(f'Device: {mx.default_device()}')"

# Verificar dados
wc -l data/train.jsonl data/valid.jsonl

# Verificar modelo
ls -lh models/mistral-7b-4bit/model.safetensors
```

### Passo 2: Abrir Notebook

```bash
cd /Users/f.nuno/Desktop/chatbot_2.0/LLM_training
jupyter notebook notebooks/mistral_qlora_training_m1_optimized.ipynb
```

### Passo 3: Executar CÃ©lulas

1. **SeÃ§Ã£o 1:** Setup (2-3 min)
   - âœ“ Imports e verificaÃ§Ãµes

2. **SeÃ§Ã£o 2:** ConfiguraÃ§Ãµes (30 seg)
   - âœ“ Mostra batch_size=4 e outros parÃ¢metros

3. **SeÃ§Ã£o 3-4:** Dados e Modelo (2-3 min)
   - âœ“ Carrega 943 exemplos
   - âœ“ Carrega Mistral-7B

4. **SeÃ§Ã£o 5-6:** TokenizaÃ§Ã£o (1 min)
   - âœ“ Converte em tokens

5. **SeÃ§Ã£o 7:** TREINO (2-3 horas) â­
   - âœ“ **Deixe rodar sem interrupÃ§Ã£o**
   - âœ“ SaÃ­da a cada 10 passos
   - âœ“ ValidaÃ§Ã£o a cada 200 passos

6. **SeÃ§Ã£o 8-10:** Testes (5 min)
   - âœ“ Testa geraÃ§Ã£o
   - âœ“ Salva modelo

### Passo 4: Monitoramento (Opcional, em Terminal Separado)

```bash
python3 scripts/monitor.py --output-dir checkpoints_qlora --refresh 5
```

Mostra em tempo real:
- Loss atual
- ValidaÃ§Ã£o loss
- MemÃ³ria usada
- ETA estimado

### Passo 5: ApÃ³s Treino

```bash
# Visualizar grÃ¡ficos
python3 scripts/visualization.py --report

# Testar modelo
python3 scripts/inference_qlora.py "Qual foi a melhor classificaÃ§Ã£o do Farense?"

# Ver mÃ©tricas
cat checkpoints_qlora/training_summary.json | jq
```

---

## ğŸ“ Estrutura Final

```
/LLM_training/
â”œâ”€â”€ CLAUDE.md                                    â† Para futuros Claude Code
â”œâ”€â”€ CONFIG_SUMMARY.txt                          â† Resumo visual (CONSULTAR)
â”œâ”€â”€ DATASET_PREPARED.md                         â† Info dataset
â”œâ”€â”€ M1_16GB_OPTIMIZATION.md                     â† Detalhes otimizaÃ§Ãµes
â”œâ”€â”€ START_TRAINING_M1.md                        â† Quick start
â”œâ”€â”€ SETUP_COMPLETE.md                           â† Este ficheiro
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.jsonl                             â† 848 exemplos (PRONTO)
â”‚   â”œâ”€â”€ valid.jsonl                             â† 95 exemplos (PRONTO)
â”‚   â””â”€â”€ farense_dataset_cleaned.jsonl           â† Backup do limpo
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ mistral_qlora_training_m1_optimized.ipynb  â† USE ESTE! â­
â”‚   â””â”€â”€ [outros notebooks]
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_qlora.py                          â† Alternativa ao notebook
â”‚   â”œâ”€â”€ split_data_proper.py                    â† Novo script
â”‚   â”œâ”€â”€ inference_qlora.py                      â† Testar modelo
â”‚   â”œâ”€â”€ monitor.py                              â† Monitoramento
â”‚   â”œâ”€â”€ visualization.py                        â† Gerar grÃ¡ficos
â”‚   â””â”€â”€ [outros scripts]
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mistral-7b-4bit/                        â† Modelo base (3.8GB)
â”‚
â”œâ”€â”€ checkpoints_qlora/                          â† SerÃ¡ criado durante treino
â”‚   â”œâ”€â”€ checkpoint_epoch*/                      â† Checkpoints
â”‚   â”œâ”€â”€ training_metrics.json                   â† MÃ©tricas
â”‚   â”œâ”€â”€ training_state.json                     â† Estado
â”‚   â””â”€â”€ adapters/                               â† Melhor modelo
â”‚
â””â”€â”€ output/
    â””â”€â”€ mistral-7b-farense-qlora/               â† SerÃ¡ criado apÃ³s treino
        â”œâ”€â”€ adapters.safetensors                â† Modelo final
        â”œâ”€â”€ adapter_config.json
        â””â”€â”€ training_config.json
```

---

## âš¡ ConfiguraÃ§Ãµes RÃ¡pidas (Se NecessÃ¡rio)

### Se Erro "Out of Memory"
```python
batch_size = 2
gradient_accumulation_steps = 4
# Effective batch = 8 (mantÃ©m-se igual)
```

### Se Loss NÃ£o Diminui
```python
learning_rate = 5e-4  # Aumentar 2.5x
num_epochs = 4  # Mais Ã©pocas
```

### Se Quer Melhor Qualidade
```python
num_epochs = 5
max_seq_length = 768
# Cuidado com memÃ³ria!
```

---

## âœ… Checklist Final

Antes de comeÃ§ar:

- â˜ Python 3.11+ instalado
- â˜ MLX com GPU detectado (`mx.default_device() = gpu`)
- â˜ Dataset em `data/train.jsonl` e `data/valid.jsonl`
- â˜ Modelo em `models/mistral-7b-4bit/` (3.8GB)
- â˜ Jupyter instalado
- â˜ Notebook aberto: `mistral_qlora_training_m1_optimized.ipynb`
- â˜ Navegador fechado (economiza ~2GB RAM)
- â˜ Outras aplicaÃ§Ãµes pesadas fechadas

---

## ğŸ“ DocumentaÃ§Ã£o de ReferÃªncia

Para consultar durante/apÃ³s treino:

1. **Quick Questions:** `CONFIG_SUMMARY.txt`
2. **ExplicaÃ§Ãµes Detalhadas:** `M1_16GB_OPTIMIZATION.md`
3. **PrÃ³ximas Etapas:** `START_TRAINING_M1.md`
4. **Info Dataset:** `DATASET_PREPARED.md`
5. **Projeto Completo:** `CLAUDE.md`

---

## ğŸš€ Pronto Para ComeÃ§ar!

Tudo estÃ¡ configurado e otimizado para seu **MacBook Pro M1 16GB**.

**Comando para comeÃ§ar:**

```bash
jupyter notebook notebooks/mistral_qlora_training_m1_optimized.ipynb
```

---

## ğŸ“Š Resultado Esperado

ApÃ³s ~2-3 horas de treino, vocÃª terÃ¡:

1. âœ… **Checkpoints** em `checkpoints_qlora/`
   - Modelos intermÃ©dios salvos a cada 200 passos
   - Possibilidade de recuperaÃ§Ã£o se falhar

2. âœ… **MÃ©tricas** em `checkpoints_qlora/training_metrics.json`
   - Loss ao longo do tempo
   - ValidaÃ§Ã£o loss
   - Timestamps e informaÃ§Ãµes de Ã©pocas

3. âœ… **Modelo Final** em `output/mistral-7b-farense-qlora/`
   - `adapters.safetensors` - Usar isto para inferÃªncia
   - `adapter_config.json` - ConfiguraÃ§Ã£o LoRA
   - `training_config.json` - HiperparÃ¢metros usados

4. âœ… **GrÃ¡ficos** em `checkpoints_qlora/plots/`
   - Loss curves (train vs validation)
   - Memory usage over time
   - Learning rate schedule

---

**Status: âœ… PRONTO PARA TREINO**

**Data:** 18 Novembro 2025  
**Hardware:** MacBook Pro M1 16GB  
**Modelo:** Mistral-7B (QLoRA)  
**Dataset:** 943 exemplos Farense  
**Tempo Estimado:** 2-3 horas  

**Boa sorte! âš½ğŸ¤–**
