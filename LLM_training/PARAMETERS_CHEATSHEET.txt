â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                            â•‘
â•‘         MACBOOK PRO M1 16GB - PARÃ‚METROS DE TREINO (CHEATSHEET)           â•‘
â•‘                                                                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


ğŸ¯ PARÃ‚METROS PRINCIPAIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€ BATCH & MEMORY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                            â”‚
â”‚  batch_size = 4                    â† CARREGA 4 EXEMPLOS POR VEZ         â”‚
â”‚  gradient_accumulation_steps = 2   â† ACUMULA 2 BATCHES ANTES ATUALIZAR  â”‚
â”‚  effective_batch_size = 8          â† COMO SE FOSSE batch_size=8         â”‚
â”‚                                                                            â”‚
â”‚  memory_per_batch = ~3-4 GB        â† MEMÃ“RIA POR BATCH                  â”‚
â”‚  total_max_memory = ~10-11 GB      â† TOTAL COM OVERHEAD                 â”‚
â”‚  available_memory = 16 GB          â† SEU MACBOOK M1                     â”‚
â”‚  safety_margin = 5-6 GB            â† MARGEM DE SEGURANÃ‡A âœ“              â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ TRAINING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                            â”‚
â”‚  learning_rate = 2e-4              â† 0.0002 (PADRÃƒO PARA LoRA)         â”‚
â”‚  num_epochs = 3                    â† 3 PASSADAS PELO DATASET             â”‚
â”‚  warmup_steps = 100                â† AQUECIMENTO DOS PRIMEIROS 100 PASSOSâ”‚
â”‚  max_seq_length = 512              â† MÃXIMO 512 TOKENS POR EXEMPLO       â”‚
â”‚                                                                            â”‚
â”‚  eval_steps = 200                  â† VALIDAÃ‡ÃƒO A CADA 200 PASSOS        â”‚
â”‚  save_steps = 200                  â† CHECKPOINT A CADA 200 PASSOS       â”‚
â”‚  log_steps = 10                    â† OUTPUT A CADA 10 PASSOS            â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ LoRA CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                            â”‚
â”‚  lora_rank = 8                     â† DECOMPOSIÃ‡ÃƒO EM 8 DIMENSÃ•ES        â”‚
â”‚  lora_scale = 16                   â† ESCALA DE ADAPTAÃ‡ÃƒO                â”‚
â”‚  lora_dropout = 0.0                â† SEM DROPOUT (DATASET PEQUENO)       â”‚
â”‚                                                                            â”‚
â”‚  target_modules = [                â† MÃ“DULOS PARA ADAPTAR:               â”‚
â”‚    "q_proj",    # Query projection                                       â”‚
â”‚    "v_proj",    # Value projection                                       â”‚
â”‚    "k_proj",    # Key projection                                         â”‚
â”‚    "o_proj",    # Output projection                                      â”‚
â”‚    "gate_proj", # Gate (MLPs)                                            â”‚
â”‚    "up_proj",   # Up projection                                          â”‚
â”‚    "down_proj"  # Down projection                                        â”‚
â”‚  ]                                                                         â”‚
â”‚                                                                            â”‚
â”‚  trainable_params = ~3.3M          â† DE 7.2B TOTAL (0.046%)             â”‚
â”‚  memory_savings = 99.95%           â† ECONOMIZA QUASE TUDO!              â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ QUANTIZATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                            â”‚
â”‚  quantization_bits = 4             â† INT4 QUANTIZATION                  â”‚
â”‚  group_size = 64                   â† TAMANHO DE GRUPO                    â”‚
â”‚                                                                            â”‚
â”‚  original_model_size = 14 GB        â† MISTRAL-7B COMPLETO               â”‚
â”‚  quantized_model_size = 3.8 GB      â† APÃ“S INT4                         â”‚
â”‚  compression_ratio = 3.7x           â† 73% REDUÃ‡ÃƒO!                      â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


âš¡ VELOCIDADE ESPERADA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

tokens_per_second = 300-500          â† VELOCIDADE DE PROCESSAMENTO
examples_per_second = 3-5            â† EXEMPLOS POR SEGUNDO
steps_per_minute = 180-300           â† PASSOS POR MINUTO

time_per_epoch = 35-40 minutes       â† TEMPO POR Ã‰POCA
total_training_time = 2-3 HOURS      â† TEMPO TOTAL (3 Ã‰POCAS)


ğŸ“ˆ TRAJETÃ“RIA DE LOSS ESPERADA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ã‰POCA 1:
  Step  10: Loss 4.85  | Val Loss 4.50  | Status: APRENDENDO
  Step  50: Loss 3.87  | Val Loss 3.65
  Step 100: Loss 3.45  | Val Loss 3.20
  Step 150: Loss 3.12  | Val Loss 3.00
  Final:    Loss ~3.0  | Val Loss ~3.0

Ã‰POCA 2:
  Step  50: Loss 2.45  | Val Loss 2.30  | Status: REFINANDO
  Step 100: Loss 2.15  | Val Loss 2.05
  Final:    Loss ~2.0  | Val Loss ~2.0

Ã‰POCA 3:
  Step  50: Loss 1.65  | Val Loss 1.70  | Status: CONSOLIDANDO
  Step 100: Loss 1.50  | Val Loss 1.65
  Final:    Loss ~1.5  | Val Loss ~1.65

INDICADORES SAUDÃVEIS:
  âœ“ Loss diminui consistentemente
  âœ“ Val Loss acompanha Train Loss
  âœ“ DiferenÃ§a < 0.2 entre train e val (sem overfitting)


ğŸ”¢ DATASET
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

total_examples = 943
train_examples = 848 (89.9%)
valid_examples = 95 (10.1%)

avg_tokens_per_example = ~30-40
min_tokens = 8 (filtrado)
max_tokens = 512 (limit)

total_train_tokens = ~27-32K
total_valid_tokens = ~3-4K


ğŸ’¾ ARQUIVO FINAL (APÃ“S TREINO)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

LOCALIZAÃ‡ÃƒO:
  output/mistral-7b-farense-qlora/
    â””â”€â”€ adapters.safetensors  â† USE ESTE PARA INFERÃŠNCIA

TAMANHO:
  adapters.safetensors â‰ˆ 50-100 MB
  adapter_config.json â‰ˆ 1 KB
  training_config.json â‰ˆ 1 KB
  
  TOTAL PARA USAR: ~100 MB (muito pequeno!)


âš™ï¸ AJUSTES SE NECESSÃRIO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SE ERRO "Out of Memory":
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  batch_size = 2           (reduzir de 4)
  gradient_accumulation_steps = 4  (aumentar de 2)
  â†’ Effective batch = 8 (mantÃ©m-se igual)
  â†’ MemÃ³ria: ~2 GB por batch

SE LOSS NÃƒO DIMINUI:
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  learning_rate = 5e-4     (aumentar 2.5x)
  num_epochs = 4-5         (mais Ã©pocas)
  â†’ MAIS AGRESSIVO MAS CUIDADO COM OVERFITTING

SE QUER MELHOR QUALIDADE:
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  num_epochs = 5           (mais Ã©pocas)
  max_seq_length = 768     (CUIDADO: pode dar OOM)
  learning_rate = 1e-4     (mais conservador)

SE GPU NÃƒO ESTÃ SENDO USADO:
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Check: python3 -c "import mlx.core as mx; print(mx.default_device())"
  Se nÃ£o for "gpu": verificar MLX installation
  Reinstalar: pip install mlx --upgrade


ğŸ“Š METRICAS MONITORADAS DURANTE TREINO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

A CADA 10 PASSOS (log_steps):
  â€¢ Loss (training loss)
  â€¢ Learning rate
  â€¢ Timestamp
  â†’ Salvo em: checkpoints_qlora/training_metrics.json

A CADA 200 PASSOS (eval_steps):
  â€¢ Validation loss
  â€¢ ComparaÃ§Ã£o train vs val
  â†’ Detecta overfitting se val >> train

A CADA 200 PASSOS (save_steps):
  â€¢ Salva checkpoint completo
  â€¢ Permite retomar se falhar
  â†’ LocalizaÃ§Ã£o: checkpoints_qlora/checkpoint_epoch*_step*/


ğŸš€ CHECKLIST PRÃ‰-TREINO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Antes de executar o notebook, verifique:

â˜ Python 3.11 ou superior
  $ python3 --version

â˜ MLX instalado e GPU detectado
  $ python3 -c "import mlx.core as mx; print(mx.default_device())"
  Deve dizer: gpu (NÃƒO cpu)

â˜ Dataset existe e Ã© vÃ¡lido
  $ wc -l data/train.jsonl data/valid.jsonl
  Deve dizer: 848 train.jsonl e 95 valid.jsonl

â˜ Modelo base existe (3.8GB)
  $ ls -lh models/mistral-7b-4bit/model.safetensors
  Deve ser ~3.8GB

â˜ Jupyter instalado
  $ jupyter --version

â˜ Notebook pronto
  $ ls notebooks/mistral_qlora_training_m1_optimized.ipynb
  Deve existir

â˜ Navegador fechado (economiza ~2GB RAM)
  Feche abas de web

â˜ Outras aplicaÃ§Ãµes pesadas fechadas
  Feche: Chrome pesado, Slack, etc.

â˜ Pronto para comeÃ§ar!
  jupyter notebook notebooks/mistral_qlora_training_m1_optimized.ipynb


ğŸ“‚ FICHEIROS GERADOS DURANTE TREINO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

checkpoints_qlora/
â”œâ”€â”€ checkpoint_epoch0_step200/      â† Checkpoint 1
â”œâ”€â”€ checkpoint_epoch0_step400/      â† Checkpoint 2
â”œâ”€â”€ ...
â”œâ”€â”€ checkpoint_epoch2_step200/      â† Checkpoint final
â”œâ”€â”€ adapters/                        â† MELHOR MODELO
â”‚   â”œâ”€â”€ adapters.safetensors
â”‚   â””â”€â”€ adapter_config.json
â”œâ”€â”€ training_metrics.json            â† DADOS TREINO (JSON)
â”œâ”€â”€ training_metrics.csv             â† DADOS TREINO (CSV)
â”œâ”€â”€ training_summary.json            â† RESUMO FINAL
â””â”€â”€ training_state.json              â† ESTADO PARA RESUMIR


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DÃšVIDAS? LEIA:
  1. START_TRAINING_M1.md (30 seg)
  2. CONFIG_SUMMARY.txt (2 min)
  3. M1_16GB_OPTIMIZATION.md (20 min)

PRONTO? COMECE:
  jupyter notebook notebooks/mistral_qlora_training_m1_optimized.ipynb

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
