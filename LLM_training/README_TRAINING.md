# ğŸš€ MacBook Pro M1 16GB - Treino do Modelo Mistral-7B QLoRA

## âš¡ Quick Start (30 segundos)

```bash
jupyter notebook notebooks/mistral_qlora_training_m1_optimized.ipynb
```

Executa as 10 seÃ§Ãµes do notebook e treina o modelo em ~2-3 horas.

---

## ğŸ“– DocumentaÃ§Ã£o Organizada

### ğŸ¯ SE VOCÃŠ QUER... LEIA ISTO:

| Necessidade | Ficheiro | Tempo |
|---|---|---|
| **ComeÃ§ar AGORA** | `START_TRAINING_M1.md` | 5 min |
| **Ver configuraÃ§Ãµes** | `CONFIG_SUMMARY.txt` | 2 min |
| **Entender tudo** | `M1_16GB_OPTIMIZATION.md` | 20 min |
| **Info do dataset** | `DATASET_PREPARED.md` | 10 min |
| **Projeto completo** | `CLAUDE.md` | 30 min |
| **PrÃ³ximas etapas** | `SETUP_COMPLETE.md` | 15 min |

---

## ğŸ¯ CONFIGURAÃ‡Ã•ES (RESUMO)

### Batch Size & Memory
```
Batch Size:                  4
Gradient Accumulation:       2
Effective Batch Size:        8

Memory Usage:
  â€¢ Modelo: 3.8 GB
  â€¢ Batch: 3-4 GB
  â€¢ Total: ~10-11 GB (de 16 GB) âœ“
```

### Training Parameters
```
Learning Rate:               2e-4
Epochs:                      3
Max Seq Length:              512
Warmup Steps:                100
```

### LoRA Configuration
```
Rank:                        8
Scale:                       16
Target Modules:              7 (q, v, k, o, gate, up, down)
```

---

## ğŸ“Š ESPERADO

### Tempo por Ã‰poca
```
Ã‰poca 1: ~40 minutos | Loss: 4.5 â†’ 3.0
Ã‰poca 2: ~40 minutos | Loss: 3.0 â†’ 2.0
Ã‰poca 3: ~40 minutos | Loss: 2.0 â†’ 1.5
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:   ~2-3 horas
```

### Velocidade
```
Tokens/segundo:              300-500
Exemplos/segundo:            3-5
Passos/minuto:               180-300
```

---

## ğŸ“š FICHEIROS DO PROJETO

### ğŸ“‚ Estrutura
```
/LLM_training/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ mistral_qlora_training_m1_optimized.ipynb  â† USE ESTE â­
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.jsonl          (848 exemplos)
â”‚   â””â”€â”€ valid.jsonl          (95 exemplos)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_qlora.py       (alternativa)
â”‚   â”œâ”€â”€ inference_qlora.py   (testar modelo)
â”‚   â”œâ”€â”€ monitor.py           (acompanhar treino)
â”‚   â””â”€â”€ visualization.py     (gerar grÃ¡ficos)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mistral-7b-4bit/     (3.8 GB)
â”‚
â”œâ”€â”€ checkpoints_qlora/       (serÃ¡ criado)
â”‚   â”œâ”€â”€ checkpoint_*/
â”‚   â”œâ”€â”€ training_metrics.json
â”‚   â””â”€â”€ adapters/
â”‚
â””â”€â”€ output/                  (serÃ¡ criado)
    â””â”€â”€ mistral-7b-farense-qlora/
        â””â”€â”€ adapters.safetensors
```

---

## ğŸš€ Passo a Passo

### 1ï¸âƒ£ VerificaÃ§Ã£o (5 min)
```bash
python3 --version  # 3.11+?
python3 -c "import mlx.core as mx; print(mx.default_device())"  # gpu?
wc -l data/train.jsonl data/valid.jsonl  # 848 + 95?
ls -lh models/mistral-7b-4bit/model.safetensors  # 3.8GB?
```

### 2ï¸âƒ£ Notebook (2-3 horas)
```bash
jupyter notebook notebooks/mistral_qlora_training_m1_optimized.ipynb
# Execute cÃ©lulas 1-10 em ordem
```

### 3ï¸âƒ£ Monitoramento (opcional)
```bash
# Em terminal separado:
python3 scripts/monitor.py --output-dir checkpoints_qlora --refresh 5
```

### 4ï¸âƒ£ Teste (5 min)
```bash
python3 scripts/inference_qlora.py "Qual foi a melhor classificaÃ§Ã£o do Farense?"
python3 scripts/visualization.py --report
```

---

## âš™ï¸ Ajustes (Se NecessÃ¡rio)

### Memory Error?
```python
batch_size = 2
gradient_accumulation_steps = 4  # MantÃ©m effective=8
```

### Loss Not Decreasing?
```python
learning_rate = 5e-4  # Aumentar 2.5x
num_epochs = 5  # Mais Ã©pocas
```

### Want Better Quality?
```python
num_epochs = 5
max_seq_length = 768  # Cuidado com memÃ³ria!
```

---

## âœ… Checklist

- â˜ Python 3.11+
- â˜ MLX GPU detectado
- â˜ Dataset pronto (train.jsonl, valid.jsonl)
- â˜ Modelo pronto (3.8GB)
- â˜ Jupyter instalado
- â˜ Navegador fechado
- â˜ AplicaÃ§Ãµes pesadas fechadas
- â˜ Pronto para comeÃ§ar!

---

## ğŸ“ PrÃ³ximas Etapas

1. âœ… Treinar modelo (~2-3 horas)
2. âœ… Validar qualidade (`inference_qlora.py`)
3. âœ… Gerar relatÃ³rios (`visualization.py`)
4. âœ… Analisar mÃ©tricas (`training_summary.json`)
5. âœ… Integrar em produÃ§Ã£o (`output/mistral-7b-farense-qlora/`)

---

## ğŸ“ DocumentaÃ§Ã£o

| Ficheiro | ConteÃºdo | Leitor |
|---|---|---|
| `START_TRAINING_M1.md` | Quick start (30 seg) | Todos |
| `CONFIG_SUMMARY.txt` | Resumo visual | Todos |
| `M1_16GB_OPTIMIZATION.md` | Guia completo | TÃ©cnico |
| `DATASET_PREPARED.md` | Info dataset | Data scientist |
| `CLAUDE.md` | Projeto completo | Developers |
| `SETUP_COMPLETE.md` | Setup detalhado | Setup |

---

## ğŸ”— ReferÃªncias RÃ¡pidas

- **Batch Size Explicado:** `M1_16GB_OPTIMIZATION.md` (seÃ§Ã£o 2.1)
- **TrajetÃ³ria Loss:** `CONFIG_SUMMARY.txt` (seÃ§Ã£o Loss Esperada)
- **Troubleshooting:** `M1_16GB_OPTIMIZATION.md` (seÃ§Ã£o 7.4)
- **Monitoramento:** `scripts/monitor.py`

---

## ğŸ“Š Dataset

- **Total:** 943 exemplos
- **Treino:** 848 (89.9%)
- **ValidaÃ§Ã£o:** 95 (10.1%)
- **Formato:** JSONL (prompt + completion)
- **DomÃ­nio:** HistÃ³ria Farense âš½
- **Qualidade:** 100% vÃ¡lido

---

## ğŸ¯ Resultado Final

ApÃ³s treino:
```
checkpoints_qlora/
â”œâ”€â”€ training_metrics.json      â† Dados de treino
â”œâ”€â”€ training_summary.json      â† Resumo
â”œâ”€â”€ adapters/                  â† Melhor modelo
â””â”€â”€ plots/                     â† GrÃ¡ficos

output/mistral-7b-farense-qlora/
â””â”€â”€ adapters.safetensors       â† Pronto para usar!
```

---

## âš¡ Performance

| MÃ©trica | Esperado |
|---|---|
| Tokens/segundo | 300-500 |
| Exemplos/segundo | 3-5 |
| Tempo/Ã©poca | 35-40 min |
| Tempo total | 2-3 horas |
| MemÃ³ria mÃ¡xima | ~11 GB |

---

## ğŸš€ Comece Agora!

```bash
jupyter notebook notebooks/mistral_qlora_training_m1_optimized.ipynb
```

---

**Hardware:** MacBook Pro M1 16GB
**Modelo:** Mistral-7B (QLoRA)
**Dataset:** 943 Exemplos Farense
**Status:** âœ… Pronto para Treino

**Boa sorte! âš½ğŸ¤–**
