â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                           â•‘
â•‘         MACBOOK PRO M1 16GB - CONFIGURAÃ‡Ã•ES DE TREINO FINAL              â•‘
â•‘                                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š RESUMO EXECUTIVO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MODELO:                  Mistral-7B v0.1 (quantizado INT4)
MÃ‰TODO:                  QLoRA (Quantized Low-Rank Adaptation)
DATASET:                 943 exemplos Farense (848 train, 95 valid)
HARDWARE:                MacBook Pro M1 16GB RAM
TEMPO ESTIMADO:          2-3 horas

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ PARÃ‚METROS PRINCIPAIS (M1 16GB OTIMIZADO)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TREINO:
  â”œâ”€ Batch Size:                    4
  â”œâ”€ Gradient Accumulation:         2
  â”œâ”€ Effective Batch Size:          8 (4 Ã— 2)
  â”œâ”€ Learning Rate:                 2e-4 (0.0002)
  â”œâ”€ NÃºmero de Ã‰pocas:              3
  â”œâ”€ Warmup Steps:                  100
  â””â”€ Max Sequence Length:           512 tokens

LoRA:
  â”œâ”€ Rank:                          8
  â”œâ”€ Scale:                         16
  â”œâ”€ Dropout:                       0.0
  â””â”€ Target Modules:                7 (q,v,k,o,gate,up,down)

CHECKPOINTING:
  â”œâ”€ Save Every:                    200 steps
  â”œâ”€ Validate Every:                200 steps
  â”œâ”€ Log Every:                     10 steps
  â””â”€ Recovery:                      AutomÃ¡tico (training_state.json)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¾ USO DE MEMÃ“RIA ESPERADO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Modelo Base (INT4):                 ~3.8 GB (permanente)
LoRA Adapters:                      ~50 MB
Batch Processing (4 exemplos):      ~3-4 GB
Otimizador + Estado:                ~1-2 GB
Overhead do Sistema:                ~2 GB
                                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total MÃ¡ximo:                       ~10-11 GB

DisponÃ­vel: 16 GB
Margem de SeguranÃ§a: 5-6 GB âœ“

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš¡ VELOCIDADE ESPERADA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Tokens por Segundo:                 300-500 tok/s
Exemplos por Segundo:               ~3-5 ex/s
Passos por Minuto:                  ~180-300 passos/min
Tempo por Ã‰poca:                    ~35-40 minutos
Tempo Total (3 Ã©pocas):             ~2-3 horas

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ˆ TRAJETÃ“RIA DE LOSS ESPERADA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ã‰POCA 1:
  Passo  10: Loss 4.85  |  Val Loss 4.50
  Passo  50: Loss 3.87  |  Val Loss 3.65
  Passo 100: Loss 3.45  |  Val Loss 3.20
  Passo 150: Loss 3.12  |  Val Loss 3.00
  Final: ~3.0

Ã‰POCA 2:
  Passo  50: Loss 2.45  |  Val Loss 2.30
  Passo 100: Loss 2.15  |  Val Loss 2.05
  Final: ~2.0

Ã‰POCA 3:
  Passo  50: Loss 1.65  |  Val Loss 1.70
  Passo 100: Loss 1.50  |  Val Loss 1.65
  Final: ~1.5

âœ“ Loss deve diminuir consistentemente
âœ“ Val_loss deve acompanhar train_loss
âœ“ Se divergir = possÃ­vel overfitting (reduzir LR)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‚ FICHEIROS IMPORTANTES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DADOS:
  âœ“ data/train.jsonl                (848 exemplos)
  âœ“ data/valid.jsonl                (95 exemplos)

MODELO:
  âœ“ models/mistral-7b-4bit/         (3.8 GB, base)

NOTEBOOK:
  âœ“ notebooks/mistral_qlora_training_m1_optimized.ipynb  (USE ESTE!)

SCRIPTS:
  âœ“ scripts/train_qlora.py           (alternativa ao notebook)
  âœ“ scripts/monitor.py               (monitoramento em tempo real)
  âœ“ scripts/visualization.py         (gerar grÃ¡ficos)
  âœ“ scripts/inference_qlora.py       (testar modelo)

OUTPUT:
  â† checkpoints_qlora/               (durante treino)
  â† output/mistral-7b-farense-qlora/ (apÃ³s treino)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸš€ COMO COMEÃ‡AR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. VERIFICAÃ‡ÃƒO PRÃ‰-TREINO (5 min):
   
   python3 --version                  # Verificar Python 3.11+
   python3 -c "import mlx.core as mx; print(mx.default_device())"  # GPU?
   wc -l data/train.jsonl data/valid.jsonl  # Dados existem?
   ls -lh models/mistral-7b-4bit/model.safetensors  # Modelo?

2. ABRIR NOTEBOOK (instantÃ¢neo):
   
   jupyter notebook notebooks/mistral_qlora_training_m1_optimized.ipynb

3. EXECUTAR CÃ‰LULAS (2-3 horas):
   
   SeÃ§Ã£o 1: Setup (2-3 min)
   SeÃ§Ã£o 2: Config (mostra batch_size=4)
   SeÃ§Ã£o 3-6: Load & Tokenize (3-5 min)
   SeÃ§Ã£o 7: TRAIN (2-3 horas) â† DEIXE RODAR
   SeÃ§Ã£o 8-10: Test & Save (5 min)

4. MONITORAR (em terminal separado):
   
   python3 scripts/monitor.py --output-dir checkpoints_qlora --refresh 5

5. TESTAR (apÃ³s treino):
   
   python3 scripts/inference_qlora.py "Qual foi..."
   python3 scripts/visualization.py --report

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš™ï¸ AJUSTES SE NECESSÃRIO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SE ERRO "Out of Memory":
  batch_size = 2  (em vez de 4)
  gradient_accumulation_steps = 4  (em vez de 2)

SE LOSS NÃƒO DIMINUI:
  learning_rate = 5e-4  (em vez de 2e-4)
  num_epochs = 4 ou 5  (em vez de 3)

SE QUER MELHOR QUALIDADE:
  num_epochs = 5  (mais Ã©pocas = melhor)
  max_seq_length = 768  (mas cuidado com memÃ³ria!)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… CHECKLIST PRÃ‰-TREINO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â˜ Python 3.11+ instalado
â˜ MLX com GPU detectado (device = gpu)
â˜ Dataset pronto (train.jsonl, valid.jsonl)
â˜ Modelo baixado (3.8GB, mistral-7b-4bit/)
â˜ Jupyter instalado
â˜ Navegador fechado (economiza memÃ³ria)
â˜ Outras aplicaÃ§Ãµes pesadas fechadas
â˜ Notebook aberto

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ PRÃ“XIMAS ETAPAS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Treinar modelo (notebook)           ~2-3 horas
2. Validar qualidade (inference)       ~5 min
3. Gerar relatÃ³rios (visualization)    ~5 min
4. Analisar mÃ©tricas (JSON/CSV)        ~5 min
5. Integrar em produÃ§Ã£o                variÃ¡vel

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ BATCH SIZE EXPLICADO (POR QUE 4?)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Batch Size = 4 significa:
â”œâ”€ Carrega 4 exemplos por vez
â”œâ”€ Faz forward/backward em 4 exemplos
â”œâ”€ Usa ~3-4 GB memÃ³ria por batch
â”œâ”€ Com gradient_accumulation=2:
â”‚  â”œâ”€ Faz 2 batches (8 exemplos)
â”‚  â”œâ”€ Depois atualiza pesos
â”‚  â””â”€ Efetivo Ã© como treinar com batch_size=8
â””â”€ Mas usa menos memÃ³ria instantÃ¢nea!

ComparaÃ§Ã£o M1:
â”œâ”€ M1 Base (8GB):    batch_size = 2
â”œâ”€ M1 Pro (16GB):    batch_size = 4  â† VOCÃŠ
â””â”€ M1 Max (32GB):    batch_size = 8

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TUDO PRONTO? COMECE COM:

jupyter notebook notebooks/mistral_qlora_training_m1_optimized.ipynb

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Criado para: MacBook Pro M1 16GB
Data: 18 Novembro 2025
Status: âœ… Pronto para Treino

Boa sorte! âš½ğŸ¤–
