# ğŸ““ Jupyter Lab Professional Training Guide

## ğŸ¯ VisÃ£o Geral

Este guia descreve como usar o **notebook profissional** (`mistral_qlora_professional.ipynb`) para treinar um modelo Mistral-7B com QLoRA em Jupyter Lab de forma segura e com seleÃ§Ã£o automÃ¡tica de configuraÃ§Ãµes.

---

## ğŸ“ Ficheiros Principais Criados

### 1. **`notebooks/mistral_qlora_professional.ipynb`** â­ PRINCIPAL
   - **Tipo:** Jupyter Notebook
   - **Tamanho:** ~100KB
   - **DuraÃ§Ã£o:** 2-3 horas (treino completo)
   - **Estrutura:** 10 cÃ©lulas temÃ¡ticas

### 2. **`scripts/visualization_professional.py`** ğŸ“Š
   - **Tipo:** Script Python standalone
   - **FunÃ§Ã£o:** Gera grÃ¡ficos matplotlib profissionais
   - **Uso:** `python3 scripts/visualization_professional.py`
   - **Output:**
     - `checkpoints_qlora/training_dashboard.png` (4 grÃ¡ficos principais)
     - `checkpoints_qlora/training_detailed_analysis.png` (6 grÃ¡ficos detalhados)

---

## ğŸš€ Como Usar o Notebook

### PrÃ©-requisitos
```bash
# Instale as dependÃªncias
pip install jupyter-lab matplotlib seaborn pandas numpy scipy

# Verifique MLX
python3 -c "import mlx.core as mx; print(mx.default_device())"
```

### Abrir o Notebook
```bash
cd /Users/f.nuno/Desktop/chatbot_2.0/LLM_training

# OpÃ§Ã£o 1: Jupyter Lab
jupyter lab notebooks/mistral_qlora_professional.ipynb

# OpÃ§Ã£o 2: Jupyter Notebook
jupyter notebook notebooks/mistral_qlora_professional.ipynb
```

---

## ğŸ“‹ Estrutura das CÃ©lulas (10 Blocos)

### **[SETUP] 1ï¸âƒ£ ImportaÃ§Ãµes e ConfiguraÃ§Ãµes Iniciais**

```
O que faz:
âœ… Importa bibliotecas (MLX, pandas, matplotlib, etc)
âœ… Configura variÃ¡veis globais (diretÃ³rios)
âœ… Define funÃ§Ãµes utilitÃ¡rias
âœ… Suprime avisos

Tempo: ~2 segundos
Pode correr isoladamente: âœ… SIM
```

**Executar primeiro!** Todas as outras cÃ©lulas dependem disto.

---

### **[SYSTEM CHECK] 2ï¸âƒ£ DiagnÃ³stico do Hardware**

```
O que faz:
âœ… Verifica Python, MLX, transformers
âœ… Detecta GPU/Metal disponÃ­vel
âœ… Mede RAM e espaÃ§o em disco
âœ… Valida arquivos de dados
âœ… Verifica modelo base

SaÃ­da:
ğŸ“Š RelatÃ³rio completo do sistema
âš ï¸  Avisos se algo faltar

Tempo: ~3-5 segundos
Pode correr isoladamente: âœ… SIM
```

**InformaÃ§Ãµes que mostra:**
- Python version
- MLX version + GPU Metal disponÃ­vel
- RAM total e disponÃ­vel
- Disco livre (GB)
- Ficheiros de dados vÃ¡lidos
- Modelo base presente

---

### **[RECOMMENDATIONS] 2.5ï¸âƒ£ RecomendaÃ§Ã£o AutomÃ¡tica de Config**

```
O que faz:
âœ… Analisa RAM disponÃ­vel
âœ… Detecta presenÃ§a de GPU
âœ… Recomenda 3 nÃ­veis de configuraÃ§Ã£o:
   1. SAFE       - MÃ­nimo seguro (sem crash)
   2. BALANCED   - Recomendado (maioria dos casos)
   3. PERFORMANCE - Para hardware superior

Tempo: ~1 segundo
Pode correr isoladamente: âœ… SIM
```

**ConfiguraÃ§Ãµes Recomendadas:**

| Config | batch_size | learning_rate | SituaÃ§Ã£o |
|--------|-----------|---|----------|
| SAFE | 1 | 0.0001 | RAM <6GB ou teste |
| BALANCED | 2 | 0.0002 | RAM 8-10GB â­ |
| PERFORMANCE | 4 | 0.0003 | RAM >12GB + GPU |

---

### **[CONFIG WIZARD] 3ï¸âƒ£ SeleÃ§Ã£o Interativa**

```
O que faz:
âœ… Oferece menu interativo (escolher 1-4)
âœ… Permite customizaÃ§Ã£o manual
âœ… Valida valores
âœ… Salva configuraÃ§Ã£o em JSON

Tempo: ~5-10 segundos (interativo)
Pode correr isoladamente: âœ… SIM

OpÃ§Ãµes:
1 = Use SAFE config
2 = Use BALANCED config (recomendado)
3 = Use PERFORMANCE config
4 = Customize manualmente cada parÃ¢metro
```

**CustomizaÃ§Ã£o Manual (OpÃ§Ã£o 4):**
- batch_size
- gradient_accumulation
- max_seq_length
- learning_rate
- num_epochs

---

### **[DATA PREP] 4ï¸âƒ£ ValidaÃ§Ã£o de Dados**

```
O que faz:
âœ… Valida formato JSONL
âœ… Verifica campos obrigatÃ³rios (prompt, completion)
âœ… Conta exemplos em train/valid
âœ… Detecta problemas (JSON invÃ¡lido, campos vazios)
âœ… Mostra estatÃ­sticas

Tempo: ~5 segundos
Pode correr isoladamente: âœ… SIM

Requerimentos:
- data/train.jsonl
- data/valid.jsonl
```

**Output esperado:**
```
âœ… train.jsonl - 850 exemplos vÃ¡lidos
âœ… valid.jsonl - 95 exemplos vÃ¡lidos
```

---

### **[MODEL SETUP] 5ï¸âƒ£ Carregamento do Modelo**

```
O que faz:
âœ… Carrega tokenizador (Mistral-7B)
âœ… Localiza modelo base quantizado (4-bit)
âœ… Configura LoRA adapters
âœ… Prepara para treino

Tempo: ~2-3 minutos
Pode correr isoladamente: âœ… SIM

Requerimentos:
- models/mistral-7b-4bit/model.safetensors (3.8GB)
```

**ConfiguraÃ§Ã£o LoRA:**
- Rank: 8
- Alpha: 16
- Target modules: q_proj, v_proj, k_proj, o_proj, gate_proj, up_proj, down_proj
- Dropout: 0.0

---

### **[TRAINING] 6ï¸âƒ£ Loop de Treino**

```
O que faz:
âœ… Treino principal (forward â†’ loss â†’ backward â†’ update)
âœ… Salva checkpoints a cada N steps
âœ… Avalia em dados de validaÃ§Ã£o
âœ… Registra mÃ©tricas em JSON Lines
âœ… Mostra progresso em barra

Tempo: ~2-3 horas (3 epochs)
Pode correr isoladamente: âŒ NÃƒO (depende de cÃ©lulas anteriores)

DependÃªncias:
- training_config (de [CONFIG WIZARD])
- tokenizer (de [MODEL SETUP])
```

**Output:**
- `checkpoints_qlora/training_metrics.json` (atualizado a cada step)
- `checkpoints_qlora/checkpoint_epochX_stepY/` (modelo em checkpoint)

---

### **[MONITORING] 7ï¸âƒ£ VisualizaÃ§Ã£o em Tempo Real**

```
O que faz:
âœ… LÃª mÃ©tricas em tempo real
âœ… Mostra grÃ¡ficos dinÃ¢micos (loss, val_loss)
âœ… Atualiza grÃ¡ficos enquanto treino roda

Tempo: ~5 segundos
Pode correr isoladamente: âœ… SIM (durante/apÃ³s treino)
```

**GrÃ¡ficos mostrados:**
- Plot 1: Loss Training vs Validation
- Plot 2: Loss mÃ©dio por Ã‰poca

---

### **[VISUALIZATION] 8ï¸âƒ£ AnÃ¡lise Detalhada (GrÃ¡ficos Profissionais)**

```
O que faz:
âœ… Gera 6 grÃ¡ficos profissionais:
   1. Loss por Ã‰poca (linhas)
   2. Volatilidade de Loss (rolling std)
   3. Taxa de Melhoria (derivada)
   4. Loss Acumulado por Ã‰poca
   5. Learning Curve (com smoothing)
   6. Box Plot (distribuiÃ§Ã£o)

âœ… Salva como PNG (150 DPI)
âœ… Pronto para apresentaÃ§Ãµes

Tempo: ~3-5 segundos
Pode correr isoladamente: âœ… SIM

Output:
- training_detailed_analysis.png
```

**Alternativa: Usar script Python**
```bash
python3 scripts/visualization_professional.py
# Gera 2 ficheiros PNG:
# - training_dashboard.png (4 grÃ¡ficos principais)
# - training_detailed_analysis.png (6 grÃ¡ficos detalhados)
```

---

### **[INFERENCE] 9ï¸âƒ£ Teste do Modelo**

```
O que faz:
âœ… Carrega modelo treinado com LoRA adapters
âœ… Tokeniza queries de teste
âœ… Gera respostas
âœ… Testa qualidade das saÃ­das

Tempo: ~2-5 segundos (por query)
Pode correr isoladamente: âœ… SIM
```

**Exemplos de teste:**
```python
"Qual foi a melhor classificaÃ§Ã£o do Farense?"
"Quantos campeonatos o Farense ganhou?"
"Quem foi o melhor treinador do Farense?"
```

---

### **[ANALYSIS] ğŸ”Ÿ Resumo Final e MÃ©tricas**

```
O que faz:
âœ… Gera relatÃ³rio final
âœ… Mostra melhorias alcanÃ§adas
âœ… Lista checkpoints salvos
âœ… PrÃ³ximos passos (deploy, etc)

Tempo: ~2 segundos
Pode correr isoladamente: âœ… SIM
```

**InformaÃ§Ãµes no relatÃ³rio:**
- DuraÃ§Ã£o total do treino
- Loss inicial vs final
- Percentual de melhoria
- Validation loss estatÃ­sticas
- AnÃ¡lise de overfitting
- LocalizaÃ§Ã£o de checkpoints

---

## ğŸ’¡ Fluxo de Uso (Primeira Vez)

### Passo 1: Abrir Jupyter Lab
```bash
jupyter lab
```

### Passo 2: Abrir Notebook
Navegue para `notebooks/mistral_qlora_professional.ipynb`

### Passo 3: Executar em Ordem
```
[SETUP] â†’ [SYSTEM CHECK] â†’ [RECOMMENDATIONS] â†’ [CONFIG WIZARD]
   â†“
[DATA PREP] â†’ [MODEL SETUP] â†’ [TRAINING] (â±ï¸ 2-3h)
   â†“
[MONITORING] â†’ [VISUALIZATION] â†’ [INFERENCE] â†’ [ANALYSIS]
```

### Passo 4: Acompanhar Progresso (Opcional)
Abra outro terminal:
```bash
python3 scripts/visualization_professional.py  # A qualquer momento
# ou
tail -f checkpoints_qlora/training_metrics.json | python3 -m json.tool
```

---

## âš¡ Dicas Importantes

### Para Evitar Crashes
- âœ… Execute [SYSTEM CHECK] primeiro
- âœ… Use config recomendada ("BALANCED")
- âœ… Se RAM < 8GB, use "SAFE" config
- âœ… Comece com 1 epoch se primeira vez

### Para Melhor Performance
- ğŸš€ Se RAM > 12GB, use "PERFORMANCE" config
- ğŸš€ Aumente batch_size incrementalmente
- ğŸš€ Use GPU Metal (MLX detecta automaticamente)
- ğŸš€ Reduza max_seq_length se treino lento

### Durante o Treino
- ğŸ“Š [MONITORING] mostra progresso em tempo real
- â¸ï¸ Pode parar com Ctrl+C (checkpoint Ã© salvo)
- ğŸ”„ Retome depois rodando [TRAINING] novamente
- ğŸ’¾ Estado salvo em `checkpoints_qlora/training_state.json`

### ApÃ³s o Treino
- ğŸ“ˆ Execute [VISUALIZATION] para grÃ¡ficos profissionais
- ğŸ”® Execute [INFERENCE] para testar modelo
- ğŸ“‹ Execute [ANALYSIS] para relatÃ³rio final
- ğŸ–¼ï¸ Use grÃ¡ficos PNG para apresentaÃ§Ãµes

---

## ğŸ›ï¸ CustomizaÃ§Ã£o de ParÃ¢metros

### Batch Size (MemÃ³ria)
```
batch_size=1 â†’ ~4-5GB RAM
batch_size=2 â†’ ~6-8GB RAM â­
batch_size=4 â†’ ~12GB+ RAM
```

### Learning Rate (Treino)
```
0.0001  â†’ Muito conservador (lento)
0.0002  â†’ Recomendado â­
0.0003  â†’ Um pouco agressivo
0.0005  â†’ Agressivo (risco instabilidade)
```

### Max Seq Length (Velocidade)
```
256   â†’ RÃ¡pido mas sequÃªncias curtas
384   â†’ Bom compromisso
512   â†’ Completo mas mais lento â­
```

### Gradient Accumulation (Batch Efetivo)
```
effective_batch = batch_size * gradient_accumulation
Exemplo: batch_size=2, grad_accum=2 â†’ effective=4
```

---

## ğŸ“Š Ficheiros Gerados

### Durante o Treino
```
checkpoints_qlora/
â”œâ”€â”€ training_metrics.json      â† Atualizado a cada step
â”œâ”€â”€ training_metrics.csv       â† Formato CSV
â”œâ”€â”€ training_state.json        â† Para retomar se interromper
â”œâ”€â”€ checkpoint_epoch0_step*/   â† Checkpoints intermediÃ¡rios
â””â”€â”€ adapters/                  â† Melhor modelo encontrado
```

### ApÃ³s Gerar GrÃ¡ficos
```
checkpoints_qlora/
â”œâ”€â”€ training_dashboard.png              â† 4 grÃ¡ficos principais
â””â”€â”€ training_detailed_analysis.png      â† 6 grÃ¡ficos detalhados
```

### ConfiguraÃ§Ã£o Usada
```
checkpoints_qlora/
â””â”€â”€ config_selected.json  â† Specs exatos do treino
```

---

## ğŸ”§ Troubleshooting

| Problema | SoluÃ§Ã£o |
|----------|---------|
| **Out of Memory** | Reduzir batch_size (2â†’1) ou aumentar gradient_accumulation (2â†’4) |
| **Treino muito lento** | Aumentar batch_size ou max_seq_length |
| **Loss nÃ£o diminui** | Aumentar learning_rate (0.0002â†’0.0003) |
| **Modelo nÃ£o carrega** | Verificar se models/mistral-7b-4bit/model.safetensors existe |
| **Dados invÃ¡lidos** | Executar [DATA PREP] para validar |
| **Metrics file nÃ£o atualiza** | Verificar se treino estÃ¡ rodando (ps aux \| grep train) |
| **GrÃ¡ficos vazios** | Executar treino primeiro (mÃ­nimo 10 steps) |

---

## ğŸ“š ReferÃªncia RÃ¡pida - Comandos

```bash
# Ver progresso do treino
tail -5 checkpoints_qlora/training_metrics.json

# Monitor em tempo real
tail -f checkpoints_qlora/training_metrics.json | python3 -m json.tool

# Gerar grÃ¡ficos profissionais
python3 scripts/visualization_professional.py

# Ver processo de treino
ps aux | grep train

# Verificar tamanho de checkpoints
du -sh checkpoints_qlora/

# Parar treino gracefully
kill -15 <PID>

# Retomar treino depois
# (Jupyter: execute [TRAINING] novamente)
```

---

## ğŸ“ˆ MÃ©tricas Esperadas

### Primeira Epoch (esperado)
- Loss inicial: ~4.5-5.0
- Loss final: ~2.5-3.0
- TendÃªncia: â†“ Decrescente âœ…

### Segunda Epoch
- Loss inicial: ~2.5-3.0
- Loss final: ~1.5-2.0
- TendÃªncia: â†“ Continua diminuindo âœ…

### Terceira Epoch
- Loss inicial: ~1.5-2.0
- Loss final: ~0.8-1.2
- TendÃªncia: â†“ PossÃ­vel convergÃªncia

### Val Loss
- Deve ser ligeiramente maior que training loss
- Se gap < 0.2 â†’ Sem overfitting âœ…
- Se gap > 0.5 â†’ Overfitting significativo âš ï¸

---

## ğŸ“ Aprender Mais

**Dentro do notebook:**
- Cada cÃ©lula tem explicaÃ§Ã£o detalhada
- Comments em cÃ³digo explicam o que faz

**Em portuguÃªs:**
- `README_PREFLIGHT.md` - Guia detalhado de preflight check
- `TRAINING_IN_PROGRESS.md` - Como monitorar durante treino

**Conceitos:**
- MLX framework: https://ml-explore.github.io/mlx/
- QLoRA: https://arxiv.org/abs/2305.14314
- LoRA: https://arxiv.org/abs/2106.09714

---

## âœ… Checklist Antes de ComeÃ§ar

- [ ] Python 3.11+
- [ ] MLX instalado (`python3 -c "import mlx"`)
- [ ] Jupyter Lab instalado
- [ ] Dados em `data/train.jsonl` e `data/valid.jsonl`
- [ ] Modelo em `models/mistral-7b-4bit/model.safetensors`
- [ ] RAM >= 6GB (8GB recomendado)
- [ ] Disco >= 20GB livre

---

**VersÃ£o:** 1.0
**Data:** 2025-11-19
**Framework:** MLX (Apple Silicon)
**Modelo:** Mistral-7B-4bit
**MÃ©todo:** QLoRA

---

**DÃºvidas?** Consulte o notebook - cada cÃ©lula estÃ¡ bem comentada!
