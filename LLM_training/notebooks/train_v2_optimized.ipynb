{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral-7B QLora Training V2 - Overfitting Reduction\n",
    "\n",
    "**Date:** 2025-11-19\n",
    "**Objective:** Reduce overfitting from V1 (gap: 2.27 ‚Üí target: < 0.15)\n",
    "**Configuration:** Optimized for better generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] Import Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Todas as bibliotecas importadas com sucesso!\n",
      "MLX Device: Device(gpu, 0)\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# MLX imports\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "from mlx.optimizers import AdamW\n",
    "from mlx_lm import load\n",
    "from mlx_lm.tuner import linear_to_lora_layers\n",
    "\n",
    "# Utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚úÖ Todas as bibliotecas importadas com sucesso!\")\n",
    "print(f\"MLX Device: {mx.default_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] Configuration V2 (Overfitting Reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéØ CONFIGURATION V2 - OVERFITTING REDUCTION\n",
      "================================================================================\n",
      "LoRA Rank: 6 (reduced from 8)\n",
      "Dropout: 0.08 (added for regularization)\n",
      "Batch Size: 2 (reduced from 4)\n",
      "Learning Rate: 2e-4 (reduced from 5e-4)\n",
      "Early Stopping: Patience=5, Min Delta=0.001\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "BASE_DIR = Path(\"/Users/f.nuno/Desktop/chatbot_2.0/LLM_training\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "CHECKPOINTS_DIR = BASE_DIR / \"checkpoints_qlora\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "\n",
    "# Create directories\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Data files\n",
    "TRAIN_FILE = DATA_DIR / \"train_v3_final_complete.jsonl\"\n",
    "VALID_FILE = DATA_DIR / \"valid_v3_final_complete.jsonl\"\n",
    "\n",
    "# Model configuration V2 - OPTIMIZED FOR OVERFITTING REDUCTION\n",
    "model_name = str(BASE_DIR / \"models/mistral-7b-4bit\")\n",
    "\n",
    "qlora_config = {\n",
    "    \"quantization\": \"int4\",\n",
    "    \"group_size\": 64,\n",
    "    \"num_layers\": 8,\n",
    "    \"lora_parameters\": {\n",
    "        \"rank\": 6,              # ‚úÖ REDUCED from 8 (25% fewer parameters)\n",
    "        \"scale\": 16,\n",
    "        \"dropout\": 0.08,        # ‚úÖ NEW (added regularization)\n",
    "        \"keys\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    },\n",
    "    \"bias\": \"none\",\n",
    "}\n",
    "\n",
    "training_config = {\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 2,            # ‚úÖ REDUCED from 4 (more regularization)\n",
    "    \"gradient_accumulation\": 4, # ‚úÖ INCREASED from 2 (maintain effective batch)\n",
    "    \"learning_rate\": 2e-4,      # ‚úÖ REDUCED from 5e-4 (slower, more stable)\n",
    "    \"max_seq_length\": 512,\n",
    "    \"warmup_steps\": 100,\n",
    "    \"save_steps\": 200,\n",
    "    \"eval_steps\": 200,\n",
    "    \"log_steps\": 10,\n",
    "    \"early_stopping_patience\": 5,      # ‚úÖ NEW\n",
    "    \"early_stopping_min_delta\": 0.001, # ‚úÖ NEW\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ CONFIGURATION V2 - OVERFITTING REDUCTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"LoRA Rank: 6 (reduced from 8)\")\n",
    "print(f\"Dropout: 0.08 (added for regularization)\")\n",
    "print(f\"Batch Size: 2 (reduced from 4)\")\n",
    "print(f\"Learning Rate: 2e-4 (reduced from 5e-4)\")\n",
    "print(f\"Early Stopping: Patience=5, Min Delta=0.001\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] Define Training Classes & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def format_prompt(sample):\n",
    "    return f\"### Pergunta:\\n{sample['prompt']}\\n\\n### Resposta:\\n{sample['completion']}\"\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def tokenize(sample, tokenizer, max_seq_length):\n",
    "    prompt_text = format_prompt(sample)\n",
    "    return tokenizer.encode(prompt_text, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "def calculate_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / (1024 ** 2)\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] MetricsTracker Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MetricsTracker class defined\n"
     ]
    }
   ],
   "source": [
    "class MetricsTracker:\n",
    "    def __init__(self, checkpoint_dir):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.metrics_file_csv = self.checkpoint_dir / \"training_metrics.csv\"\n",
    "        self.metrics_file_json = self.checkpoint_dir / \"training_metrics.json\"\n",
    "        self.summary_file = self.checkpoint_dir / \"training_summary.json\"\n",
    "        self.training_state_file = self.checkpoint_dir / \"training_state.json\"\n",
    "        self.best_model_path = self.checkpoint_dir / \"adapters\" / \"adapters.safetensors\"\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.metrics_data = []\n",
    "        self.start_time = time.time()\n",
    "        self.current_epoch = 0\n",
    "        self.current_step = 0\n",
    "\n",
    "    def log_step(self, epoch, step, loss, val_loss=None, memory_mb=None, learning_rate=None):\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - self.start_time\n",
    "        \n",
    "        metric = {\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": step,\n",
    "            \"loss\": loss.item() if hasattr(loss, 'item') else loss,\n",
    "            \"timestamp\": current_time,\n",
    "            \"elapsed_time_sec\": elapsed_time,\n",
    "            \"memory_mb\": memory_mb if memory_mb is not None else calculate_memory_usage(),\n",
    "            \"learning_rate\": learning_rate if learning_rate is not None else training_config[\"learning_rate\"],\n",
    "        }\n",
    "        if val_loss is not None:\n",
    "            metric[\"val_loss\"] = val_loss.item() if hasattr(val_loss, 'item') else val_loss\n",
    "\n",
    "        self.metrics_data.append(metric)\n",
    "        \n",
    "        with open(self.metrics_file_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.metrics_data, f, indent=4, ensure_ascii=False)\n",
    "        pd.DataFrame(self.metrics_data).to_csv(self.metrics_file_csv, index=False)\n",
    "        self.current_epoch = epoch\n",
    "        self.current_step = step\n",
    "\n",
    "    def save_best_model(self, model, val_loss):\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            adapters_dir = self.checkpoint_dir / \"adapters\"\n",
    "            adapters_dir.mkdir(parents=True, exist_ok=True)\n",
    "            model.save_weights(str(self.best_model_path))\n",
    "            print(f\"‚úì Melhor modelo guardado com Val Loss: {self.best_val_loss:.4f}\")\n",
    "\n",
    "    def save_summary(self, total_time, total_samples):\n",
    "        summary = {\n",
    "            \"total_training_time_sec\": total_time,\n",
    "            \"total_samples_processed\": total_samples,\n",
    "            \"final_epoch\": self.current_epoch,\n",
    "            \"final_step\": self.current_step,\n",
    "            \"best_validation_loss\": self.best_val_loss,\n",
    "            \"training_config\": training_config,\n",
    "            \"qlora_config\": qlora_config,\n",
    "        }\n",
    "        with open(self.summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ MetricsTracker class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5] EarlyStoppingMonitor Class ‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EarlyStoppingMonitor class defined\n"
     ]
    }
   ],
   "source": [
    "class EarlyStoppingMonitor:\n",
    "    \"\"\"Monitora overfitting e aplica early stopping autom√°tico\"\"\"\n",
    "\n",
    "    def __init__(self, patience=5, min_delta=0.001, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        self.best_epoch = 0\n",
    "        self.best_step = 0\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.should_stop = False\n",
    "        self.overfitting_gap_history = []\n",
    "\n",
    "    def check(self, val_loss, train_loss, epoch, step):\n",
    "        \"\"\"Verifica se deve parar o treino\"\"\"\n",
    "        gap = val_loss - train_loss\n",
    "        self.overfitting_gap_history.append(gap)\n",
    "\n",
    "        if gap > 0.30:\n",
    "            print(f\"  ‚ö†Ô∏è  OVERFITTING SEVERO DETECTADO (gap={gap:.4f})\")\n",
    "        elif gap > 0.15:\n",
    "            print(f\"  ‚ö†Ô∏è  Overfitting moderado (gap={gap:.4f})\")\n",
    "\n",
    "        if val_loss < self.best_val_loss - self.min_delta:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.patience_counter = 0\n",
    "            self.best_epoch = epoch\n",
    "            self.best_step = step\n",
    "            print(f\"  ‚úÖ Melhor Val Loss: {self.best_val_loss:.4f}\")\n",
    "            return False, True\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            if self.patience_counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "                print(f\"\\n‚èπÔ∏è  EARLY STOPPING ATIVADO!\")\n",
    "                print(f\"   Sem melhoria por {self.patience} valida√ß√µes consecutivas\")\n",
    "                print(f\"   Melhor modelo: √âpoca {self.best_epoch}, Step {self.best_step}\")\n",
    "                print(f\"   Melhor Val Loss: {self.best_val_loss:.4f}\")\n",
    "                return True, False\n",
    "            else:\n",
    "                print(f\"  ‚ÑπÔ∏è  Sem melhoria ({self.patience_counter}/{self.patience})\")\n",
    "                return False, False\n",
    "\n",
    "    def get_overfitting_status(self):\n",
    "        \"\"\"Retorna status de overfitting\"\"\"\n",
    "        if not self.overfitting_gap_history:\n",
    "            return \"Sem dados\"\n",
    "\n",
    "        avg_gap = sum(self.overfitting_gap_history) / len(self.overfitting_gap_history)\n",
    "\n",
    "        if avg_gap < 0.05:\n",
    "            return \"‚úÖ EXCELENTE (gap < 0.05)\"\n",
    "        elif avg_gap < 0.15:\n",
    "            return \"‚úÖ BOM (gap < 0.15)\"\n",
    "        elif avg_gap < 0.30:\n",
    "            return \"‚ö†Ô∏è MODERADO (gap < 0.30)\"\n",
    "        else:\n",
    "            return \"‚ùå CR√çTICO (gap >= 0.30)\"\n",
    "\n",
    "print(\"‚úÖ EarlyStoppingMonitor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [6] Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loss functions defined\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(model, inputs, targets, lengths):\n",
    "    mask = mx.arange(inputs.shape[1])[None, :] < lengths[:, None]\n",
    "    logits = model(inputs)\n",
    "    loss = nn.losses.cross_entropy(logits, targets, reduction='none')\n",
    "    loss = mx.sum(loss * mask) / mx.sum(mask)\n",
    "    return loss, logits\n",
    "\n",
    "def create_step_fn(model, optimizer):\n",
    "    grad_fn = nn.value_and_grad(model, loss_fn)\n",
    "    def step_fn(inputs, targets, lengths):\n",
    "        (loss, logits), grads = grad_fn(model, inputs, targets, lengths)\n",
    "        optimizer.update(model, grads)\n",
    "        return loss\n",
    "    return step_fn\n",
    "\n",
    "def create_eval_fn(model):\n",
    "    def eval_fn(inputs, targets, lengths):\n",
    "        loss, _ = loss_fn(model, inputs, targets, lengths)\n",
    "        return loss\n",
    "    return eval_fn\n",
    "\n",
    "print(\"‚úÖ Loss functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [7] Load Model & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ A carregar modelo e dados...\n",
      "A carregar modelo: /Users/f.nuno/Desktop/chatbot_2.0/LLM_training/models/mistral-7b-4bit\n",
      "\n",
      "üìä A carregar datasets...\n",
      "Amostras de treino: 848\n",
      "Amostras de valida√ß√£o: 95\n",
      "\n",
      "üî§ A tokenizar...\n",
      "‚úÖ Tokens de treino: 848\n",
      "‚úÖ Tokens de valida√ß√£o: 95\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüì¶ A carregar modelo e dados...\")\n",
    "print(f\"A carregar modelo: {model_name}\")\n",
    "\n",
    "model, tokenizer = load(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "lora_only_config = {k: v for k, v in qlora_config.items() if k not in [\"quantization\", \"group_size\"]}\n",
    "linear_to_lora_layers(\n",
    "    model,\n",
    "    lora_only_config[\"num_layers\"],\n",
    "    lora_only_config[\"lora_parameters\"],\n",
    ")\n",
    "\n",
    "print(\"\\nüìä A carregar datasets...\")\n",
    "train_dataset = load_dataset(TRAIN_FILE)\n",
    "val_dataset = load_dataset(VALID_FILE)\n",
    "\n",
    "print(f\"Amostras de treino: {len(train_dataset)}\")\n",
    "print(f\"Amostras de valida√ß√£o: {len(val_dataset)}\")\n",
    "\n",
    "print(\"\\nüî§ A tokenizar...\")\n",
    "train_tokens = [tokenize(sample, tokenizer, training_config[\"max_seq_length\"]) for sample in train_dataset]\n",
    "val_tokens = [tokenize(sample, tokenizer, training_config[\"max_seq_length\"]) for sample in val_dataset]\n",
    "\n",
    "train_tokens = [t for t in train_tokens if t]\n",
    "val_tokens = [t for t in val_tokens if t]\n",
    "\n",
    "print(f\"‚úÖ Tokens de treino: {len(train_tokens)}\")\n",
    "print(f\"‚úÖ Tokens de valida√ß√£o: {len(val_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [8] TRAINING LOOP V2 ‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ INICIANDO TREINO V2 - OVERFITTING REDUCTION\n",
      "================================================================================\n",
      "Total de passos de treino esperados: 1272\n",
      "\n",
      "üìö √âpoca 1/3\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Treino:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 199/424 [3:52:51<5:29:38, 87.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Val Loss (step 200): 1.4834\n",
      "‚úì Melhor modelo guardado com Val Loss: 1.4834\n",
      "\n",
      "üìä An√°lise de Valida√ß√£o:\n",
      "  ‚úÖ Melhor Val Loss: 1.4834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treino:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 200/424 [4:25:09<40:00:03, 642.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Checkpoint guardado em: /Users/f.nuno/Desktop/chatbot_2.0/LLM_training/checkpoints_qlora/checkpoint_epoch0_step200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treino:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 399/424 [9:08:17<33:47, 81.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Val Loss (step 400): 1.3087\n",
      "‚úì Melhor modelo guardado com Val Loss: 1.3087\n",
      "\n",
      "üìä An√°lise de Valida√ß√£o:\n",
      "  ‚ö†Ô∏è  Overfitting moderado (gap=0.1524)\n",
      "  ‚úÖ Melhor Val Loss: 1.3087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treino:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 400/424 [9:39:35<4:08:07, 620.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Checkpoint guardado em: /Users/f.nuno/Desktop/chatbot_2.0/LLM_training/checkpoints_qlora/checkpoint_epoch0_step400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treino: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 424/424 [10:14:07<00:00, 86.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìö √âpoca 2/3\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treino:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå               | 199/424 [4:56:01<5:53:48, 94.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Val Loss (step 200): 1.3034\n",
      "‚úì Melhor modelo guardado com Val Loss: 1.3034\n",
      "\n",
      "üìä An√°lise de Valida√ß√£o:\n",
      "  ‚úÖ Melhor Val Loss: 1.3034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treino:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 200/424 [5:31:04<43:21:03, 696.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Checkpoint guardado em: /Users/f.nuno/Desktop/chatbot_2.0/LLM_training/checkpoints_qlora/checkpoint_epoch1_step200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treino:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 399/424 [9:39:30<40:37, 97.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Val Loss (step 400): 1.2482\n",
      "‚úì Melhor modelo guardado com Val Loss: 1.2482\n",
      "\n",
      "üìä An√°lise de Valida√ß√£o:\n",
      "  ‚úÖ Melhor Val Loss: 1.2482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treino:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 399/424 [10:15:58<38:35, 92.63s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[write] Unable to write 29360128 bytes to file.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     83\u001b[39m         checkpoint_path = CHECKPOINTS_DIR / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcheckpoint_epoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_step\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m         checkpoint_path.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m         \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madapters.safetensors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Checkpoint guardado em: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m early_stopping.should_stop:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/chatbot_2.0/LLM_training/mlx_kernel_env/lib/python3.13/site-packages/mlx/nn/layers/base.py:220\u001b[39m, in \u001b[36mModule.save_weights\u001b[39m\u001b[34m(self, file)\u001b[39m\n\u001b[32m    218\u001b[39m     mx.savez(file, **params_dict)\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m file.endswith(\u001b[33m\"\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     \u001b[43mmx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_safetensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    223\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported file extension for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Use \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.npz\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    224\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: [write] Unable to write 29360128 bytes to file."
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ INICIANDO TREINO V2 - OVERFITTING REDUCTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "optimizer = AdamW(learning_rate=training_config[\"learning_rate\"])\n",
    "tracker = MetricsTracker(CHECKPOINTS_DIR)\n",
    "early_stopping = EarlyStoppingMonitor(\n",
    "    patience=training_config[\"early_stopping_patience\"],\n",
    "    min_delta=training_config[\"early_stopping_min_delta\"]\n",
    ")\n",
    "\n",
    "train_step_fn = create_step_fn(model, optimizer)\n",
    "eval_fn = create_eval_fn(model)\n",
    "\n",
    "total_train_steps = (len(train_tokens) // training_config[\"batch_size\"]) * training_config[\"num_epochs\"]\n",
    "print(f\"Total de passos de treino esperados: {total_train_steps}\")\n",
    "\n",
    "for epoch in range(training_config[\"num_epochs\"]):\n",
    "    random.shuffle(train_tokens)\n",
    "    \n",
    "    print(f\"\\nüìö √âpoca {epoch+1}/{training_config['num_epochs']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i in tqdm(range(len(train_tokens) // training_config[\"batch_size\"]), desc=f\"Treino\"):\n",
    "        batch_start = i * training_config[\"batch_size\"]\n",
    "        batch_end = (i + 1) * training_config[\"batch_size\"]\n",
    "        batch_tokens = train_tokens[batch_start:batch_end]\n",
    "\n",
    "        max_len = max(len(t) for t in batch_tokens)\n",
    "        inputs = mx.array([t + [0] * (max_len - len(t)) for t in batch_tokens])\n",
    "        targets = inputs\n",
    "        lengths = mx.array([len(t) for t in batch_tokens])\n",
    "\n",
    "        loss = train_step_fn(inputs, targets, lengths)\n",
    "        mx.eval(model.parameters(), optimizer.state, loss)\n",
    "        \n",
    "        if (i + 1) % training_config[\"log_steps\"] == 0:\n",
    "            mem_usage = calculate_memory_usage()\n",
    "            tracker.log_step(epoch, i + 1, loss, memory_mb=mem_usage)\n",
    "\n",
    "        # Avalia√ß√£o\n",
    "        if (i + 1) % training_config[\"eval_steps\"] == 0 and val_tokens:\n",
    "            val_loss_sum = 0\n",
    "            num_val_batches = len(val_tokens) // training_config[\"batch_size\"]\n",
    "            if num_val_batches == 0 and len(val_tokens) > 0:\n",
    "                num_val_batches = 1\n",
    "\n",
    "            for j in range(num_val_batches):\n",
    "                val_batch_start = j * training_config[\"batch_size\"]\n",
    "                val_batch_end = (j + 1) * training_config[\"batch_size\"]\n",
    "                val_batch_tokens = val_tokens[val_batch_start:val_batch_end]\n",
    "                \n",
    "                if not val_batch_tokens:\n",
    "                    continue\n",
    "\n",
    "                val_max_len = max(len(t) for t in val_batch_tokens)\n",
    "                val_inputs = mx.array([t + [0] * (val_max_len - len(t)) for t in val_batch_tokens])\n",
    "                val_targets = val_inputs\n",
    "                val_lengths = mx.array([len(t) for t in val_batch_tokens])\n",
    "\n",
    "                val_loss = eval_fn(val_inputs, val_targets, val_lengths)\n",
    "                val_loss_sum += val_loss.item()\n",
    "            \n",
    "            avg_val_loss = val_loss_sum / num_val_batches if num_val_batches > 0 else float('inf')\n",
    "            print(f\"\\n‚úÖ Val Loss (step {i+1}): {avg_val_loss:.4f}\")\n",
    "            tracker.log_step(epoch, i + 1, loss, val_loss=avg_val_loss, memory_mb=calculate_memory_usage())\n",
    "            tracker.save_best_model(model, avg_val_loss)\n",
    "\n",
    "            # Early Stopping Check\n",
    "            print(f\"\\nüìä An√°lise de Valida√ß√£o:\")\n",
    "            should_stop, improved = early_stopping.check(\n",
    "                val_loss=avg_val_loss,\n",
    "                train_loss=loss.item(),\n",
    "                epoch=epoch,\n",
    "                step=i + 1\n",
    "            )\n",
    "\n",
    "            if should_stop:\n",
    "                print(f\"\\nüèÅ Treino terminado por Early Stopping\")\n",
    "                break\n",
    "\n",
    "        if (i + 1) % training_config[\"save_steps\"] == 0:\n",
    "            checkpoint_path = CHECKPOINTS_DIR / f\"checkpoint_epoch{epoch}_step{i+1}\"\n",
    "            checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "            model.save_weights(str(checkpoint_path / \"adapters.safetensors\"))\n",
    "            print(f\"‚úì Checkpoint guardado em: {checkpoint_path}\")\n",
    "\n",
    "    if early_stopping.should_stop:\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TREINO COMPLETO!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [9] Final Analysis & Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# An√°lise Final de Overfitting\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç AN√ÅLISE FINAL DE OVERFITTING\")\n",
    "print(\"=\"*80)\n",
    "overfitting_status = early_stopping.get_overfitting_status()\n",
    "print(f\"Status: {overfitting_status}\")\n",
    "if early_stopping.overfitting_gap_history:\n",
    "    avg_gap = sum(early_stopping.overfitting_gap_history) / len(early_stopping.overfitting_gap_history)\n",
    "    max_gap = max(early_stopping.overfitting_gap_history)\n",
    "    min_gap = min(early_stopping.overfitting_gap_history)\n",
    "    print(f\"Gap m√©dio: {avg_gap:.4f}\")\n",
    "    print(f\"Gap m√°ximo: {max_gap:.4f}\")\n",
    "    print(f\"Gap m√≠nimo: {min_gap:.4f}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Salvar modelo final\n",
    "print(\"\\n--- Guardando modelo final ---\")\n",
    "final_model_path = OUTPUT_DIR / \"mistral-7b-farense-qlora-v2\"\n",
    "final_model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "shutil.copy(tracker.best_model_path, final_model_path / \"adapters.safetensors\")\n",
    "\n",
    "with open(final_model_path / \"adapter_config.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(qlora_config, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "tracker.save_summary(time.time() - tracker.start_time, len(train_dataset))\n",
    "\n",
    "print(f\"\\n‚úÖ Modelo guardado em: {final_model_path}\")\n",
    "print(f\"‚úÖ M√©tricas guardadas em: {CHECKPOINTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10] Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "with open(CHECKPOINTS_DIR / \"training_metrics.json\") as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss trajectory\n",
    "ax = axes[0, 0]\n",
    "ax.plot(df['step'], df['loss'], 'b-', label='Training Loss', alpha=0.7)\n",
    "if 'val_loss' in df.columns:\n",
    "    val_steps = df[df['val_loss'].notna()]['step']\n",
    "    val_losses = df[df['val_loss'].notna()]['val_loss']\n",
    "    ax.scatter(val_steps, val_losses, color='r', s=50, label='Validation Loss')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training & Validation Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss per epoch\n",
    "ax = axes[0, 1]\n",
    "for epoch in df['epoch'].unique():\n",
    "    epoch_data = df[df['epoch'] == epoch]\n",
    "    ax.plot(epoch_data['step'], epoch_data['loss'], label=f'Epoch {int(epoch)+1}')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss by Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Memory usage\n",
    "ax = axes[1, 0]\n",
    "ax.plot(df['step'], df['memory_mb'], 'g-', alpha=0.7)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Memory (MB)')\n",
    "ax.set_title('Memory Usage')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting gap\n",
    "ax = axes[1, 1]\n",
    "if early_stopping.overfitting_gap_history:\n",
    "    ax.plot(range(len(early_stopping.overfitting_gap_history)), early_stopping.overfitting_gap_history, 'o-', color='purple')\n",
    "    ax.axhline(y=0.15, color='r', linestyle='--', label='Warning Threshold (0.15)')\n",
    "    ax.axhline(y=0.30, color='darkred', linestyle='--', label='Critical Threshold (0.30)')\n",
    "    ax.set_xlabel('Validation Check')\n",
    "    ax.set_ylabel('Overfitting Gap (Val - Train)')\n",
    "    ax.set_title('Overfitting Gap Detection')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CHECKPOINTS_DIR / 'training_v2_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [11] Comparison V1 vs V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPARISON: V1 vs V2\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = {\n",
    "    'Metric': ['F-1 Score', 'Train-Val Gap', 'Overfitting Status', 'Training Duration'],\n",
    "    'V1': ['0.9602', '2.27 (HIGH)', 'HIGH', '~4 hours'],\n",
    "    'V2 (This Run)': [\n",
    "        'TBD',\n",
    "        f'{avg_gap:.4f}' if 'avg_gap' in locals() else 'TBD',\n",
    "        overfitting_status,\n",
    "        f'{(time.time() - tracker.start_time) / 3600:.1f} hours'\n",
    "    ],\n",
    "    'Target': ['‚â•0.93', '<0.15', 'GOOD/EXCELENTE', 'Similar']\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLX + PyTorch (Python 3.13)",
   "language": "python",
   "name": "mlx_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
