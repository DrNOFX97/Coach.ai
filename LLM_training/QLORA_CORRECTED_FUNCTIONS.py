"""
QLoRA Training - Corrected Functions
=====================================

This file contains the corrected versions of the training functions
that fix the freezing issue at step 0.

Location: Cell 19 in mistral_qlora_training.ipynb
Date: 2025-11-09
Issue: Training freezing at 0% due to incorrect loss computation

Key Fixes:
1. Use nn.losses.cross_entropy() instead of manual log_softmax
2. Force evaluation with mx.eval() to prevent lazy computation buildup
3. Properly average gradients before applying
4. Apply remaining gradients at end of epoch
5. Better error handling with verbose logging
"""

import mlx.core as mx
import mlx.nn as nn
from tqdm import tqdm


def train_epoch(model, train_dataset, optimizer, epoch, config, tracker, memory_monitor):
    """
    QLoRA training with gradient accumulation and memory cleanup - FIXED

    Args:
        model: MLX model with QLoRA adapters
        train_dataset: Dataset of training examples
        optimizer: MLX optimizer (usually Adam)
        epoch: Current epoch number (0-indexed)
        config: Training configuration dict
        tracker: TrainingTracker for checkpoints
        memory_monitor: MemoryMonitor for cleanup

    Returns:
        float: Average loss for the epoch

    Key Improvements:
    - Uses nn.losses.cross_entropy() for stable loss computation
    - Forces evaluation with mx.eval() to prevent memory buildup
    - Properly averages accumulated gradients
    - Applies remaining gradients at epoch end
    - Verbose error logging with flush=True
    """
    print(f"\nEpoch {epoch + 1}/{config['num_epochs']}")
    total_loss = 0
    num_batches = 0
    accumulated_grads = None
    accumulation_step = 0

    num_steps = len(train_dataset) // config['batch_size']
    memory_monitor.log_memory(f"Epoch {epoch + 1} start")

    for step in tqdm(range(num_steps), desc="Training", leave=False):
        try:
            # Check if memory is critically low
            if memory_monitor.check_critical():
                print(f"  [WARN] Skipping step {step} - critical memory")
                continue

            # Get batch indices
            batch_indices = list(range(
                step * config['batch_size'],
                min((step + 1) * config['batch_size'], len(train_dataset))
            ))

            step_loss = 0
            batch_count = 0

            # Process each example in batch
            for idx in batch_indices:
                try:
                    item = train_dataset[idx]
                    input_ids = mx.array(item['input_ids']).astype(mx.int32)

                    # Forward pass with CORRECTED loss computation
                    def loss_fn(model):
                        """
                        Compute cross-entropy loss for causal language modeling

                        CRITICAL FIX: Use nn.losses.cross_entropy() instead of
                        manual log_softmax computation which was causing:
                        - Numerical instability
                        - NaN gradients
                        - Training freezes
                        - Incorrect loss values
                        """
                        try:
                            # Get logits from model: [batch, seq_len, vocab_size]
                            logits = model(input_ids.reshape(1, -1))

                            # Verify logits shape
                            if logits.size == 0:
                                return mx.array(0.0)

                            if len(logits.shape) == 3:
                                # Shift for next-token prediction:
                                # - Logits: predict tokens [1:] based on input [:-1]
                                # - Labels: actual tokens [1:]
                                shift_logits = logits[:, :-1, :]  # Remove last prediction
                                shift_labels = input_ids[1:]      # Remove first label

                                # Reshape for cross_entropy:
                                # - logits: [batch * seq_len, vocab_size]
                                # - labels: [batch * seq_len]
                                logits_flat = shift_logits.reshape(-1, shift_logits.shape[-1])
                                labels_flat = shift_labels.reshape(-1)

                                # Use built-in cross_entropy (stable, optimized)
                                # This replaces the broken manual log_softmax:
                                #   max_logits = mx.max(shift_logits, axis=-1, keepdims=True)
                                #   numerator = shift_logits - max_logits
                                #   denominator = mx.log(mx.sum(mx.exp(numerator), axis=-1, keepdims=True))
                                #   log_probs = numerator - denominator
                                #   loss = -mx.mean(log_probs)  # WRONG!
                                loss = nn.losses.cross_entropy(
                                    logits_flat,
                                    labels_flat,
                                    reduction="mean"
                                )
                            else:
                                # Fallback if unexpected shape
                                loss = mx.mean(logits)

                            return loss
                        except Exception as e:
                            print(f"    [ERROR loss_fn] {str(e)[:100]}", flush=True)
                            return mx.array(0.0)

                    # Compute loss and gradients
                    try:
                        loss_val, grads = mx.value_and_grad(loss_fn)(model)

                        # CRITICAL: Force evaluation to prevent lazy computation buildup
                        mx.eval(loss_val)
                        loss_float = float(loss_val)

                        # Check for NaN or explosion
                        if loss_float != loss_float or loss_float > 1e6:
                            print(f"    [WARN] Invalid loss: {loss_float}", flush=True)
                            continue

                        step_loss += loss_float
                        batch_count += 1

                        # Accumulate gradients (QLoRA optimization)
                        if accumulated_grads is None:
                            accumulated_grads = grads
                        else:
                            # Sum gradients across accumulation steps
                            for key in accumulated_grads:
                                if key in grads:
                                    accumulated_grads[key] = accumulated_grads[key] + grads[key]

                        accumulation_step += 1

                        # Update weights after accumulation
                        if accumulation_step >= config['gradient_accumulation']:
                            if accumulated_grads is not None:
                                # CRITICAL: Average gradients before applying
                                # This was missing in the broken version!
                                for key in accumulated_grads:
                                    accumulated_grads[key] = accumulated_grads[key] / config['gradient_accumulation']

                                # Apply gradients
                                optimizer.update(model, accumulated_grads)
                                mx.eval(model)

                                # Reset accumulation
                                accumulated_grads = None
                                accumulation_step = 0

                    except Exception as e:
                        print(f"    [ERROR gradient] {str(e)[:100]}", flush=True)
                        continue

                except Exception as e:
                    print(f"    [ERROR batch] {str(e)[:100]}", flush=True)
                    continue

            # Update loss
            if batch_count > 0:
                avg_step_loss = step_loss / batch_count
                total_loss += avg_step_loss
                num_batches += 1

            # Periodic cleanup
            if (step + 1) % config.get('memory_cleanup_steps', 5) == 0:
                memory_monitor.cleanup()

            # Logging
            if (step + 1) % config['logging_steps'] == 0:
                avg_loss = total_loss / num_batches if num_batches > 0 else 0
                memory_monitor.log_memory(f"Step {step + 1}")
                print(f"  Step {step + 1}/{num_steps} - Loss: {avg_loss:.4f}", flush=True)

            # Checkpoint
            if (step + 1) % config['save_steps'] == 0:
                checkpoint_loss = total_loss / num_batches if num_batches > 0 else 0
                tracker.save_checkpoint(model, epoch, step + 1, checkpoint_loss)
                print(f"  âœ“ Checkpoint saved (step {step + 1})", flush=True)

        except Exception as e:
            print(f"  [ERROR step {step}] {str(e)[:100]}", flush=True)
            continue

    # CRITICAL: Apply any remaining accumulated gradients
    # This was missing in the broken version!
    if accumulation_step > 0 and accumulated_grads is not None:
        print(f"  [INFO] Applying {accumulation_step} remaining gradients", flush=True)
        for key in accumulated_grads:
            accumulated_grads[key] = accumulated_grads[key] / accumulation_step
        optimizer.update(model, accumulated_grads)
        mx.eval(model)

    # Final cleanup
    memory_monitor.cleanup()
    memory_monitor.log_memory(f"Epoch {epoch + 1} end")

    avg_epoch_loss = total_loss / num_batches if num_batches > 0 else 0
    print(f"  Epoch {epoch + 1} - Avg Loss: {avg_epoch_loss:.4f}")
    return avg_epoch_loss


def validate_model(model, val_dataset, config, memory_monitor):
    """
    Validation with memory cleanup - CORRECTED VERSION

    Args:
        model: MLX model with QLoRA adapters
        val_dataset: Dataset of validation examples
        config: Training configuration dict
        memory_monitor: MemoryMonitor for cleanup

    Returns:
        float: Average validation loss

    Key Improvements:
    - Uses same loss computation as training (nn.losses.cross_entropy)
    - Forces evaluation with mx.eval()
    - Better error handling
    """
    total_loss = 0
    num_batches = 0
    num_steps = min(len(val_dataset) // config['batch_size'], 30)

    memory_monitor.log_memory("Validation start")

    for step in tqdm(range(num_steps), desc="Validation", leave=False):
        try:
            if memory_monitor.check_critical():
                break

            batch_indices = list(range(
                step * config['batch_size'],
                min((step + 1) * config['batch_size'], len(val_dataset))
            ))

            for idx in batch_indices:
                try:
                    item = val_dataset[idx]
                    input_ids = mx.array(item['input_ids']).astype(mx.int32)

                    # Forward only (no gradients)
                    try:
                        logits = model(input_ids.reshape(1, -1))

                        if logits.size > 0 and len(logits.shape) == 3:
                            # FIXED: Use same loss computation as training
                            shift_logits = logits[:, :-1, :]
                            shift_labels = input_ids[1:]

                            logits_flat = shift_logits.reshape(-1, shift_logits.shape[-1])
                            labels_flat = shift_labels.reshape(-1)

                            loss = nn.losses.cross_entropy(
                                logits_flat,
                                labels_flat,
                                reduction="mean"
                            )
                        else:
                            loss = mx.array(0.0)

                        # CRITICAL: Force evaluation
                        mx.eval(loss)
                        loss_val = float(loss)

                        # Check for NaN
                        if loss_val == loss_val and loss_val < 1e6:
                            total_loss += loss_val
                            num_batches += 1
                    except Exception as e:
                        continue
                except:
                    continue

            if (step + 1) % 10 == 0:
                memory_monitor.cleanup()

        except Exception as e:
            continue

    memory_monitor.cleanup()
    memory_monitor.log_memory("Validation end")

    return total_loss / num_batches if num_batches > 0 else 0


# Quick reference for the key fix
LOSS_COMPUTATION_FIX = """
BEFORE (BROKEN):
---------------
max_logits = mx.max(shift_logits, axis=-1, keepdims=True)
numerator = shift_logits - max_logits
denominator = mx.log(mx.sum(mx.exp(numerator), axis=-1, keepdims=True))
log_probs = numerator - denominator
loss = -mx.mean(log_probs)

Problems:
- No label indexing
- Numerically unstable
- Creates complex computation graph
- Causes training to freeze

AFTER (FIXED):
-------------
shift_logits = logits[:, :-1, :]
shift_labels = input_ids[1:]

logits_flat = shift_logits.reshape(-1, shift_logits.shape[-1])
labels_flat = shift_labels.reshape(-1)

loss = nn.losses.cross_entropy(
    logits_flat,
    labels_flat,
    reduction="mean"
)

Benefits:
- Proper label handling
- Numerically stable
- Optimized for Metal GPU
- Training proceeds smoothly
"""

if __name__ == "__main__":
    print("QLoRA Training - Corrected Functions")
    print("=" * 50)
    print("\nThese functions fix the training freeze issue.")
    print("Copy them to Cell 19 in mistral_qlora_training.ipynb")
    print("\nKey fixes:")
    print("1. Use nn.losses.cross_entropy() for loss")
    print("2. Force evaluation with mx.eval()")
    print("3. Average gradients before applying")
    print("4. Apply remaining gradients at epoch end")
    print("\nLoss Computation Fix:")
    print(LOSS_COMPUTATION_FIX)
