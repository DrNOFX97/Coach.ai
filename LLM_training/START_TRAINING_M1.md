# ğŸš€ COMECE A TREINAR EM 30 SEGUNDOS

## Quick Start para MacBook Pro M1 16GB

### âœ… PRÃ‰-REQUISITOS (Verifique)

```bash
# 1. Verificar Python 3.11+
python3 --version

# 2. Verificar MLX GPU
python3 -c "import mlx.core as mx; print(f'âœ“ Device: {mx.default_device()}')"

# 3. Verificar dados (devem existir)
wc -l data/train.jsonl data/valid.jsonl
# Esperado: 848 train.jsonl, 95 valid.jsonl

# 4. Verificar modelo (3.8GB)
ls -lh models/mistral-7b-4bit/model.safetensors
```

---

## ğŸ¯ OPÃ‡ÃƒO 1: TREINO INTERATIVO (Recomendado)

### 1. Abrir Jupyter
```bash
jupyter notebook notebooks/mistral_qlora_training_m1_optimized.ipynb
```

### 2. Executar Todas as CÃ©lulas em Ordem
- SeÃ§Ã£o 1: Setup (2-3 min)
- SeÃ§Ã£o 2: ConfiguraÃ§Ãµes (mostra batch_size=4)
- SeÃ§Ã£o 3-4: Carregar dados e modelo (2-3 min)
- SeÃ§Ã£o 5-6: TokenizaÃ§Ã£o e setup (1 min)
- **SeÃ§Ã£o 7: TREINO (2-3 horas)** â­
- SeÃ§Ã£o 8-10: Testes e resultados (5 min)

**Tempo Total:** ~2.5-3.5 horas

---

## ğŸ¯ OPÃ‡ÃƒO 2: TREINO VIA SCRIPT

```bash
# Executar treino completo via script
python3 scripts/train_qlora.py

# Isto vai:
# 1. Carregar dados
# 2. Carregar modelo
# 3. Tokenizar
# 4. Treinar 3 Ã©pocas
# 5. Salvar checkpoints
# 6. Gerar mÃ©tricas
```

---

## ğŸ“Š CONFIGURAÃ‡Ã•ES (M1 16GB)

| ParÃ¢metro | Valor | Notas |
|-----------|-------|-------|
| **Batch Size** | **4** | Seguro para M1 16GB |
| **Gradient Accumulation** | **2** | Effective batch = 8 |
| **Learning Rate** | **2e-4** | PadrÃ£o para LoRA |
| **Epochs** | **3** | ~40 min cada |
| **Max Seq Length** | **512** | Suporta 512 tokens |

**Se der erro de memÃ³ria:**
```python
batch_size = 2  # Reduzir para isto
gradient_accumulation_steps = 4
```

---

## ğŸ“ˆ DURANTE O TREINO

### Terminal Separado: Monitorar Progresso

```bash
python3 scripts/monitor.py --output-dir checkpoints_qlora --refresh 5
```

Mostra:
- Loss atual
- ValidaÃ§Ã£o loss
- MemÃ³ria usada
- Tokens por segundo
- ETA estimado

### Output Esperado no Notebook

```
ğŸš€ INICIANDO TREINO
==========================================================================

ğŸ“ Ã‰POCA 1/3
----------------------------------------------------------------------
  Passo  10 | Loss: 4.8523
  Passo  20 | Loss: 4.5432
  Passo  30 | Loss: 4.2341
  ...
  Passo 200 | Loss: 3.1234
  [INFO] Avaliando em validaÃ§Ã£o...
  âœ“ Val Loss: 3.0123

âœ“ Ã‰poca 1 concluÃ­da em 2145.3s
  Loss mÃ©dio: 3.5678

ğŸ“ Ã‰POCA 2/3
...

âœ“ TREINO COMPLETO em 2.1 horas
```

---

## ğŸ“‚ FICHEIROS GERADOS

ApÃ³s treino, vocÃª terÃ¡:

```
checkpoints_qlora/
â”œâ”€â”€ training_metrics.json       â† Dados de treino
â”œâ”€â”€ training_summary.json       â† Resumo
â”œâ”€â”€ checkpoint_epoch*_step*/    â† Checkpoints
â””â”€â”€ adapters/                   â† Melhor modelo
    â””â”€â”€ adapters.safetensors

output/mistral-7b-farense-qlora/
â””â”€â”€ adapters.safetensors        â† USE ESTE PARA INFERÃŠNCIA
```

---

## ğŸ§ª APÃ“S TREINO: TESTAR MODELO

### Teste RÃ¡pido
```bash
python3 scripts/inference_qlora.py "Qual foi a melhor classificaÃ§Ã£o do Farense?"
```

Output esperado:
```json
{
  "prompt": "Qual foi a melhor classificaÃ§Ã£o do Farense?",
  "response": "O Farense teve sua melhor classificaÃ§Ã£o...",
  "method": "QLoRA",
  "status": "success"
}
```

### Visualizar GrÃ¡ficos
```bash
python3 scripts/visualization.py --report
```

Gera:
- `loss_curves.png` - Loss over time
- `learning_rate.png` - LR schedule
- `memory_usage.png` - Memory tracking

---

## âš¡ DICAS RÃPIDAS

1. **Feche navegador** antes de treinar (economiza 2GB)
2. **NÃ£o toque no notebook** durante treino (deixe rodar)
3. **Use monitor.py** em terminal separado para acompanhar
4. **Salvar checkpoints** permite retomar se falhar
5. **Se OOM:** Reduzir batch_size de 4 para 2

---

## ğŸ†˜ ERROS COMUNS

### "Out of Memory"
```
SoluÃ§Ã£o: batch_size = 2 (em vez de 4)
```

### Loss stuck at 4.5
```
SoluÃ§Ã£o: learning_rate = 5e-4 (em vez de 2e-4)
```

### GPU not detected
```
Verifique: python3 -c "import mlx.core as mx; print(mx.default_device())"
Deve dizer: gpu
```

---

## ğŸ“ PRÃ“XIMAS ETAPAS

ApÃ³s treino bem-sucedido:

1. âœ… Validar qualidade (rodar inference_qlora.py)
2. âœ… Analisar mÃ©tricas (ver training_summary.json)
3. âœ… Gerar relatÃ³rios (scripts/visualization.py)
4. âœ… Integrar em aplicaÃ§Ã£o (usar output/mistral-7b-farense-qlora/)

---

## ğŸ“ DOCUMENTAÃ‡ÃƒO COMPLETA

Para detalhes tÃ©cnicos, leia:
- `M1_16GB_OPTIMIZATION.md` - ConfiguraÃ§Ãµes detalhadas
- `DATASET_PREPARED.md` - InformaÃ§Ãµes do dataset
- `CLAUDE.md` - Guia completo do projeto
- `docs/DOCS_INDEX.md` - Todos os guias

---

**Tudo pronto? Abra o notebook e comece! ğŸš€**

```bash
jupyter notebook notebooks/mistral_qlora_training_m1_optimized.ipynb
```

Boa sorte! âš½ğŸ¤–
