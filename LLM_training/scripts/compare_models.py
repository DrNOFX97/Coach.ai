#!/usr/bin/env python3
"""
Compare LoRA vs QLoRA models for Farense Bot
ExecuÃ§Ã£o: python scripts/compare_models.py
"""

import json
import time
from pathlib import Path
from datetime import datetime

try:
    from mlx_lm import load, generate
except ImportError:
    print("Error: mlx-lm not installed. Run: pip install mlx mlx-lm")
    exit(1)

# Model paths
LORA_MODEL_PATH = "/Users/f.nuno/Desktop/chatbot_2.0/LLM_training/output/mistral-7b-farense-lora"
QLORA_MODEL_PATH = "/Users/f.nuno/Desktop/chatbot_2.0/LLM_training/output/mistral-7b-farense-qlora"
BASE_MODEL = "mistralai/Mistral-7B-v0.1"

# Test prompts
TEST_PROMPTS = [
    "Qual foi a melhor classificaÃ§Ã£o do Farense?",
    "Conte-me sobre Hassan Nader",
    "Qual Ã© a histÃ³ria do Sporting Clube Farense?",
]

class ModelComparator:
    def __init__(self):
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "models": {},
            "comparison": {}
        }

    def load_model(self, name, model_path, quantization=None):
        """Load a model"""
        print(f"\nðŸ“¦ Loading {name}...")
        try:
            if quantization:
                model, tokenizer = load(
                    BASE_MODEL,
                    adapter_path=model_path,
                    quantization=quantization
                )
                print(f"  âœ“ Loaded with {quantization} quantization")
            else:
                model, tokenizer = load(
                    BASE_MODEL,
                    adapter_path=model_path
                )
                print(f"  âœ“ Loaded without quantization")

            return model, tokenizer
        except Exception as e:
            print(f"  âœ— Error loading {name}: {e}")
            return None, None

    def benchmark_model(self, name, model, tokenizer, prompts, max_tokens=150):
        """Benchmark a model"""
        print(f"\nâš¡ Benchmarking {name}...")

        results = {
            "name": name,
            "responses": [],
            "timing": [],
            "avg_tokens_per_sec": 0,
            "total_time": 0
        }

        for i, prompt in enumerate(prompts, 1):
            print(f"  [{i}/{len(prompts)}] {prompt[:50]}...", end=" ", flush=True)

            try:
                start_time = time.time()
                response = generate(
                    model,
                    tokenizer,
                    prompt=prompt,
                    max_tokens=max_tokens,
                    verbose=False
                )
                inference_time = time.time() - start_time

                tokens_generated = len(tokenizer.encode(response))
                tokens_per_sec = tokens_generated / inference_time if inference_time > 0 else 0

                results["responses"].append({
                    "prompt": prompt,
                    "response": response[:200],  # Truncate for storage
                    "tokens": tokens_generated,
                    "time_seconds": inference_time,
                    "tokens_per_sec": tokens_per_sec
                })

                results["timing"].append(inference_time)

                print(f"âœ“ ({inference_time:.2f}s, {tokens_per_sec:.0f} t/s)")

            except Exception as e:
                print(f"âœ— Error: {str(e)[:50]}")
                results["responses"].append({
                    "prompt": prompt,
                    "error": str(e)
                })

        if results["timing"]:
            results["total_time"] = sum(results["timing"])\n            results["avg_time"] = sum(results["timing"]) / len(results["timing"])
            results["avg_tokens_per_sec"] = sum(
                r["tokens_per_sec"] for r in results["responses"] if "tokens_per_sec" in r
            ) / len([r for r in results["responses"] if "tokens_per_sec" in r])

        return results

    def compare(self):
        """Run full comparison"""
        print("=" * 70)
        print("MODEL COMPARISON: LoRA vs QLoRA")
        print("=" * 70)

        # Load models
        lora_model, lora_tokenizer = self.load_model(
            "LoRA",
            LORA_MODEL_PATH,
            quantization=None
        )

        qlora_model, qlora_tokenizer = self.load_model(
            "QLoRA",
            QLORA_MODEL_PATH,
            quantization="int4"
        )

        if not lora_model or not qlora_model:
            print("\nâœ— Could not load one or both models")
            return

        # Benchmark
        lora_results = self.benchmark_model(
            "LoRA",
            lora_model,
            lora_tokenizer,
            TEST_PROMPTS
        )

        qlora_results = self.benchmark_model(
            "QLoRA",
            qlora_model,
            qlora_tokenizer,
            TEST_PROMPTS
        )

        # Store results
        self.results["models"]["lora"] = lora_results
        self.results["models"]["qlora"] = qlora_results

        # Compare
        self.print_comparison(lora_results, qlora_results)

        # Save results
        self.save_results()

    def print_comparison(self, lora_results, qlora_results):
        \"\"\"Print comparison table\"\"\"
        print("\n" + "=" * 70)
        print("COMPARISON RESULTS")
        print("=" * 70)

        lora_time = lora_results.get("avg_time", 0)
        qlora_time = qlora_results.get("avg_time", 0)
        lora_speed = lora_results.get("avg_tokens_per_sec", 0)
        qlora_speed = qlora_results.get("avg_tokens_per_sec", 0)

        speedup = (lora_time - qlora_time) / lora_time * 100 if lora_time > 0 else 0

        print(f"\n{'Metric':<30} {'LoRA':<20} {'QLoRA':<20}\"")
        print("-" * 70)
        print(f"{'Avg Response Time (s)':<30} {lora_time:<20.2f} {qlora_time:<20.2f}\")
        print(f"{'Avg Speed (tokens/sec)':<30} {lora_speed:<20.0f} {qlora_speed:<20.0f}\")
        print(f"{'Total Time for Tests (s)':<30} {lora_results['total_time']:<20.2f} {qlora_results['total_time']:<20.2f}\")
        print(f"{'Responses Generated':<30} {len(lora_results['responses']):<20} {len(qlora_results['responses']):<20}\")
        print("-" * 70)

        if speedup > 0:
            print(f"\nâœ“ QLoRA is {speedup:.1f}% {'faster' if speedup > 0 else 'slower'} than LoRA\")

        print("\nðŸ“Š MODEL SPECS:")
        print("  LoRA:")
        print("    Size: 14GB")
        print("    VRAM: 8-10GB")
        print("  QLoRA:")
        print("    Size: 3.5GB (75% reduction)")
        print("    VRAM: 4-6GB (40% reduction)")

        print("\nâœ“ Recommendation: Use QLoRA for Mac M1 in production")

    def save_results(self):
        \"\"\"Save results to JSON\"\"\"
        output_path = Path("/Users/f.nuno/Desktop/chatbot_2.0/LLM_training/output/comparison_results.json\")
        with open(output_path, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        print(f"\nâœ“ Results saved to {output_path}\")

def main():
    try:
        comparator = ModelComparator()
        comparator.compare()
    except KeyboardInterrupt:
        print("\nâœ— Comparison interrupted by user\")
    except Exception as e:
        print(f"\nâœ— Error: {e}\")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
