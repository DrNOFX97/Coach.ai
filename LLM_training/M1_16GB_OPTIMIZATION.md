# MacBook Pro M1 16GB - ConfiguraÃ§Ãµes Otimizadas âš¡

## ğŸ¯ Resumo Executivo

Seu notebook foi otimizado especificamente para **MacBook Pro M1 16GB** com as seguintes configuraÃ§Ãµes:

| ParÃ¢metro | Valor | RazÃ£o |
|-----------|-------|-------|
| **Batch Size** | **4** | M1 16GB pode lidar confortavelmente com batches de 4 |
| **Gradient Accumulation** | **2** | Simula batch size de 8 com economia de memÃ³ria |
| **Effective Batch Size** | **8** | batch_size Ã— accumulation_steps |
| **Learning Rate** | **2e-4** | PadrÃ£o para LoRA fine-tuning |
| **Epochs** | **3** | Suficiente para dataset de 943 exemplos |
| **Max Sequence Length** | **512** | Suporta textos atÃ© ~512 tokens |
| **Warmup Steps** | **100** | Aquecimento do LR nos primeiros 100 passos |

---

## ğŸ“Š Detalhes das ConfiguraÃ§Ãµes

### 1. PARÃ‚METROS DE TREINO

```python
# BATCH SIZE EXPLICADO
batch_size = 4
gradient_accumulation_steps = 2

# Isto significa:
# - VocÃª carrega 4 exemplos por vez (reduzido para caber em memÃ³ria)
# - Faz forward/backward em 4 exemplos
# - ApÃ³s 2 iteraÃ§Ãµes (total 8 exemplos), atualiza os pesos
# - Efetivamente treina com batch_size=8, mas usa 4x menos memÃ³ria
```

**Por que este tamanho?**
- M1 16GB Ã© poderoso mas quantizado (INT4)
- Batch size 4 = ~6-7GB de memÃ³ria durante treino
- Batch size 6+ = risco de out-of-memory
- Gradient accumulation permite effective batch = 8 sem overflow

### 2. TAXA DE APRENDIZAGEM (Learning Rate)

```
Learning Rate: 2e-4 (0.0002)

ProgressÃ£o tÃ­pica (com warmup):
Passo 1-100:   LR sobe de 0 â†’ 2e-4
Passo 101+:    LR = 2e-4 (constante)
```

**Este valor Ã© bom para:**
- Fine-tuning de modelos prÃ©-treinados âœ“
- LoRA com low-rank decomposition âœ“
- Dataset pequeno (943 exemplos) âœ“

### 3. NÃšMERO DE Ã‰POCAS

```
Ã‰pocas: 3

Passar 3 vezes pelo dataset completo:
â”œâ”€ Ã‰poca 1: Aprender padrÃµes gerais
â”œâ”€ Ã‰poca 2: Refinar conhecimento
â””â”€ Ã‰poca 3: Consolidar aprendizagem
```

**Tempo esperado:**
- Ã‰poca 1: ~35-40 minutos
- Ã‰poca 2: ~35-40 minutos
- Ã‰poca 3: ~35-40 minutos
- **Total: ~2-3 horas**

### 4. COMPRIMENTO MÃXIMO DE SEQUÃŠNCIA

```
Max Sequence Length: 512 tokens

TokenizaÃ§Ã£o tÃ­pica:
"Qual foi o resultado do Farense contra X em YYYY-MM-DD?"
"O Farense jogou fora de casa e o resultado foi 1-0"
= ~30-40 tokens

MÃ¡ximo por exemplo: 512 tokens (~2000 caracteres)
```

### 5. CHECKPOINTING

```
Save Checkpoint Every: 200 passos
Evaluate Every: 200 passos
Log Every: 10 passos

Com ~210 passos por Ã©poca:
â”œâ”€ Checkpoint apÃ³s ~95% de cada Ã©poca
â”œâ”€ ValidaÃ§Ã£o a cada 200 passos
â””â”€ Permite retomar se falhar
```

---

## ğŸ› ï¸ CONFIGURAÃ‡Ã•ES LoRA

```python
# LoRA = Low-Rank Adaptation
# Treina apenas ~0.1% dos parÃ¢metros do modelo

lora_rank = 8              # DecomposiÃ§Ã£o em 8 dimensÃµes
lora_scale = 16            # Escala de adaptaÃ§Ã£o
lora_dropout = 0.0         # Sem dropout (dataset pequeno)

target_modules = [
    "q_proj",      # Query projection
    "v_proj",      # Value projection
    "k_proj",      # Key projection
    "o_proj",      # Output projection
    "gate_proj",   # Gate (for MLPs)
    "up_proj",     # Up projection
    "down_proj"    # Down projection
]
```

**ParÃ¢metros treinÃ¡veis:**
- Modelo completo: ~7.2 bilhÃµes
- Com LoRA: ~3.3 milhÃµes (0.046%)
- Economia: 99.95%! ğŸš€

---

## ğŸ’¾ MEMÃ“RIA E DESEMPENHO

### Uso de MemÃ³ria Esperado

```
Modelo base (INT4):          ~3.8 GB (permanente)
LoRA Adapters:                ~50 MB
Batch (4 exemplos):           ~3-4 GB
Otimizador + estado:          ~1-2 GB
Overhead do sistema:          ~2 GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total mÃ¡ximo:                 ~10-11 GB

DisponÃ­vel: 16 GB
Margem: 5-6 GB âœ“ (seguro!)
```

### Velocidade Esperada

```
Tokens processados por segundo: 300-500 tok/s
Exemplos por segundo: ~3-5 exemplos/s
Passos por minuto: ~180-300 passos/min
Tempo por Ã©poca: ~35-40 minutos
```

---

## ğŸ“ˆ TRAJETÃ“RIA DE LOSS ESPERADA

```
Durante o treino, vocÃª verÃ¡:

Ã‰POCA 1:
  Passo  10 | Loss: 4.85
  Passo  20 | Loss: 4.32
  Passo  50 | Loss: 3.87
  Passo 100 | Loss: 3.45
  Passo 150 | Loss: 3.12
  âœ“ ValidaÃ§Ã£o: val_loss â‰ˆ 3.0

Ã‰POCA 2:
  Passo  10 | Loss: 3.00
  Passo  50 | Loss: 2.45
  Passo 100 | Loss: 2.15
  âœ“ ValidaÃ§Ã£o: val_loss â‰ˆ 2.0

Ã‰POCA 3:
  Passo  10 | Loss: 1.95
  Passo  50 | Loss: 1.65
  Passo 100 | Loss: 1.50
  âœ“ ValidaÃ§Ã£o: val_loss â‰ˆ 1.65

Espera-se uma reduÃ§Ã£o SUAVE e consistente.
Se loss ficar preso, pode aumentar LR para 5e-4.
```

---

## ğŸš€ COMO USAR O NOTEBOOK

### Passo 1: Abrir Notebook
```bash
jupyter notebook notebooks/mistral_qlora_training_m1_optimized.ipynb
```

### Passo 2: Executar CÃ©lulas na Ordem

**SeÃ§Ã£o 1 (2-3 min):** Setup e verificaÃ§Ã£o
- âœ“ Carrega imports
- âœ“ Verifica GPU Metal
- âœ“ Verifica memÃ³ria disponÃ­vel

**SeÃ§Ã£o 2 (instantÃ¢neo):** Mostrar configuraÃ§Ãµes
- âœ“ Exibe todas as configuraÃ§Ãµes (batch size, LR, etc)
- âœ“ Confirma que M1 foi detectado

**SeÃ§Ã£o 3-4 (1-2 min):** Carregar dados e modelo
- âœ“ Carrega 848 exemplos de treino
- âœ“ Carrega 95 exemplos de validaÃ§Ã£o
- âœ“ Carrega Mistral-7B (pode levar 1-2 min primeira vez)

**SeÃ§Ã£o 5 (30 seg):** TokenizaÃ§Ã£o
- âœ“ Converte texto em tokens
- âœ“ Mostra estatÃ­sticas de tamanho

**SeÃ§Ã£o 6 (instantÃ¢neo):** Inicializar tracker
- âœ“ Prepara sistema de mÃ©tricas

**SeÃ§Ã£o 7 (2-3 horas):** â­ MAIN TRAINING LOOP
- Isto Ã© o nÃºcleo do treino
- VocÃª verÃ¡ progresso a cada 10 passos
- Deixe rodar (nÃ£o feche o notebook)

**SeÃ§Ã£o 8 (1-2 min):** Testes de geraÃ§Ã£o
- âœ“ Testa qualidade do modelo
- âœ“ Mostra exemplos de respostas

**SeÃ§Ã£o 9 (instantÃ¢neo):** Salvar modelo
- âœ“ Salva resumo de treino
- âœ“ Mostra prÃ³ximas etapas

---

## âš™ï¸ AJUSTES FINOS (Se NecessÃ¡rio)

### Se Receber Erro de MemÃ³ria:

```python
# REDUZIR BATCH SIZE (atual: 4)
batch_size = 2  # Reduzido
gradient_accumulation_steps = 4  # Aumentado para compensar

# Effective batch size mantÃ©m-se = 8
# Mas usa menos memÃ³ria instantÃ¢nea
```

### Se Loss NÃ£o Diminuir:

```python
# AUMENTAR LEARNING RATE
learning_rate = 5e-4  # De 2e-4 para 5e-4

# Mais agressivo mas pode overfitar
# Monitor validaÃ§Ã£o loss atentamente
```

### Se Quiser Treinar Mais:

```python
# AUMENTAR Ã‰POCAS
num_epochs = 4 ou 5

# Tempo total aumentarÃ¡ proporcionalmente
# Cada Ã©poca â‰ˆ 40 minutos
```

### Se Quiser Melhor Qualidade:

```python
# AUMENTAR MAX_SEQ_LENGTH
max_seq_length = 768  # De 512

# Permite exemplos mais longos
# Usa mais memÃ³ria (cuidado!)
# Provavelmente vai dar OOM, nÃ£o recomendado
```

---

## ğŸ“Š MONITORAMENTO EM TEMPO REAL

### Terminal Separado

Enquanto o notebook treina, abra outro terminal:

```bash
python3 scripts/monitor.py --output-dir checkpoints_qlora --refresh 5
```

Isto mostra em tempo real:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TRAINING PROGRESS (Updated every 5s)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Current Loss: 2.45
Best Val Loss: 1.98
Memory Used: 7.8 / 16.0 GB
Tokens/sec: 420
Steps/sec: 3.2
ETA: 45 min remaining
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ“ˆ APÃ“S TREINO: PRÃ“XIMOS PASSOS

### 1. Visualizar Resultados
```bash
python3 scripts/visualization.py --report
```
Gera grÃ¡ficos de loss, memÃ³ria, taxa de aprendizagem.

### 2. Testar Modelo
```bash
python3 scripts/inference_qlora.py "Qual foi a melhor classificaÃ§Ã£o do Farense?"
```

### 3. Analisar MÃ©tricas
```bash
cat checkpoints_qlora/training_summary.json | jq
```

### 4. Comparar VersÃµes
```bash
python3 scripts/compare_models.py
```

---

## ğŸ” FICHEIROS IMPORTANTES

### ApÃ³s treino, vocÃª terÃ¡:

```
checkpoints_qlora/
â”œâ”€â”€ training_metrics.json        â† MÃ©tricas detalhadas (JSON)
â”œâ”€â”€ training_metrics.csv         â† MÃ©tricas (CSV)
â”œâ”€â”€ training_summary.json        â† Resumo final
â”œâ”€â”€ training_state.json          â† Estado para retomar
â”œâ”€â”€ checkpoint_epoch0_step200/   â† Checkpoints intermÃ©dios
â”œâ”€â”€ checkpoint_epoch1_step200/
â”œâ”€â”€ checkpoint_epoch2_step200/
â”œâ”€â”€ adapters/                    â† Melhor modelo
â”‚   â””â”€â”€ adapters.safetensors
â””â”€â”€ plots/                       â† VisualizaÃ§Ãµes (se geradas)
    â”œâ”€â”€ loss_curves.png
    â”œâ”€â”€ learning_rate.png
    â””â”€â”€ memory_usage.png

output/mistral-7b-farense-qlora/
â”œâ”€â”€ adapters.safetensors         â† Usar isto para inferÃªncia
â”œâ”€â”€ adapter_config.json
â””â”€â”€ training_config.json
```

---

## âœ… CHECKLIST PRÃ‰-TREINO

Antes de executar o notebook, verifique:

```bash
# âœ“ Verificar Python
python3 --version  # Deve ser 3.11+

# âœ“ Verificar MLX GPU
python3 -c "import mlx.core as mx; print(f'Device: {mx.default_device()}')"

# âœ“ Verificar dados
wc -l data/train.jsonl data/valid.jsonl

# âœ“ Verificar modelo
ls -lh models/mistral-7b-4bit/model.safetensors

# âœ“ Verificar espaÃ§o em disco
df -h /Users/f.nuno/Desktop/chatbot_2.0/LLM_training/

# âœ“ Fechar outras aplicaÃ§Ãµes (especialmente navegador com muitos tabs)
```

---

## ğŸ†˜ TROUBLESHOOTING

### "Out of Memory" Error

**Causa:** Batch size demasiado grande para seu M1

**SoluÃ§Ã£o:**
```python
batch_size = 2  # em vez de 4
gradient_accumulation_steps = 4  # em vez de 2
```

### Loss nÃ£o diminui (stuck at 4.5)

**Causa:** Learning rate demasiado baixa

**SoluÃ§Ã£o:**
```python
learning_rate = 5e-4  # em vez de 2e-4
```

### Training muito lento (<100 tokens/sec)

**Causa:** GPU nÃ£o estÃ¡ sendo usada, pode estar em CPU mode

**Verificar:**
```python
import mlx.core as mx
print(mx.default_device())  # Deve ser "gpu"
```

### Notebook congela

**Causa:** Batch muito grande ou exemplos muito longos

**SoluÃ§Ã£o:** Reiniciar notebook e reduzir batch_size ou max_seq_length

---

## ğŸ’¡ DICAS PROFISSIONAIS

1. **Feche o navegador** antes de treinar - economiza ~2GB RAM
2. **Use o monitor.py** em terminal separado - vÃª progresso em tempo real
3. **Salve checkpoints frequentemente** - permite recuperaÃ§Ã£o se falhar
4. **Teste com batch_size=4 primeiro** - ajuste depois se necessÃ¡rio
5. **Monitore val_loss** - se subir enquanto train_loss desce = overfitting
6. **Guarde o melhor modelo** - usar adapters/ em vez do Ãºltimo checkpoint

---

## ğŸ“Š ESPECIFICAÃ‡Ã•ES DO SEU HARDWARE

```
MacBook Pro M1 16GB
â”œâ”€ Chip: Apple Silicon M1
â”œâ”€ Cores CPU: 8 (4 performance + 4 efficiency)
â”œâ”€ Cores GPU: 7 ou 8 (Metal Performance Shaders)
â”œâ”€ RAM: 16 GB (unified memory)
â”œâ”€ Storage: SSD (variÃ¡vel)
â””â”€ MLX Framework: Otimizado para M1 âœ“

ComparaÃ§Ã£o:
- M1 Base (8GB):     batch_size=2
- M1 Pro (16GB):     batch_size=4  â† VOCÃŠ ESTÃ AQUI
- M1 Max (32GB):     batch_size=8
- M2/M3:             Similar ao M1
```

---

## ğŸ“ PRÃ“XIMAS MELHORIAS

ApÃ³s treinar com sucesso, pode:

1. **Aumentar dataset** - Adicionar mais exemplos Farense
2. **Fine-tune no adaptador** - Treinar mais 1-2 Ã©pocas
3. **Testar hyperparameters** - Experimentar LR, batch_size
4. **Integrar em aplicaÃ§Ã£o** - Usar inference_qlora.py em produÃ§Ã£o
5. **Continuar treino** - Recuperar do checkpoint_epoch2_step*

---

**Criado para:** MacBook Pro M1 16GB
**Data:** 18 Novembro 2025
**Status:** Pronto para uso ğŸš€

Boa sorte com o treino! âš½ğŸ¤–
